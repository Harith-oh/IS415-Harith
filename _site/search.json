[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this section, I will install and load tidyverse and sf packages.\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting the Geospatial Data",
    "text": "Plotting the Geospatial Data\n\nplot(mpsz)\n\n\n\n\n#Importing polyline feature data in shapefile form\n\ncyclingpath = st_read(dsn = \"Data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\Harith-oh\\IS415-Harith\\Hands-on_Ex\\Hands-on_Ex01\\Data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2248 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n#Importing GIS data in kml format\n\npreschool = st_read(\"Data/geospatial/preschools-location.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\Harith-oh\\IS415-Harith\\Hands-on_Ex\\Hands-on_Ex01\\Data\\geospatial\\preschools-location.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n#Working with st_geometry()\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n#Working with glimpse()\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n#Working with head()\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nplot(st_geometry(mpsz))\n\n\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n#Assigning EPSG code to a simple feature data frame\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\npreschool3414 <- st_transform(preschool, crs = 3414)\n\n#Importing the aspatial data\n\nlistings <-read_csv((\"Data/aspatial/listings.csv\"))\n\n\nlist(listings)\n\n[[1]]\n# A tibble: 4,161 × 18\n       id name     host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n    <dbl> <chr>      <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  50646 Pleasan…  227796 Sujatha Centra… Bukit …    1.33    104. Privat…    80\n 2  71609 Ensuite…  367042 Belinda East R… Tampin…    1.35    104. Privat…   145\n 3  71896 B&B  Ro…  367042 Belinda East R… Tampin…    1.35    104. Privat…    85\n 4  71903 Room 2-…  367042 Belinda East R… Tampin…    1.35    104. Privat…    85\n 5 275344 15 mins… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    49\n 6 289234 Booking…  367042 Belinda East R… Tampin…    1.34    104. Privat…   184\n 7 294281 5 mins … 1521514 Elizab… Centra… Newton     1.31    104. Privat…    79\n 8 324945 Cozy Bl… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    49\n 9 330089 Cozy Bl… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    55\n10 330095 10 mins… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    55\n# … with 4,151 more rows, 8 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>,\n#   number_of_reviews_ltm <dbl>, license <chr>, and abbreviated variable names\n#   ¹​host_name, ²​neighbourhood_group, ³​neighbourhood, ⁴​latitude, ⁵​longitude,\n#   ⁶​room_type\n\n\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\n\nglimpse(listings_sf)\n\nRows: 4,161\nColumns: 17\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275344, 289…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 145, 85, 85, 49, 184, 79, 49, 55, 5…\n$ minimum_nights                 <dbl> 92, 92, 92, 92, 60, 92, 92, 60, 60, 60,…\n$ number_of_reviews              <dbl> 18, 20, 24, 47, 14, 12, 133, 17, 12, 3,…\n$ last_review                    <date> 2014-12-26, 2020-01-17, 2019-10-13, 20…\n$ reviews_per_month              <dbl> 0.18, 0.15, 0.18, 0.34, 0.11, 0.10, 1.0…\n$ calculated_host_listings_count <dbl> 1, 6, 6, 6, 44, 6, 7, 44, 44, 44, 6, 7,…\n$ availability_365               <dbl> 365, 340, 265, 365, 296, 285, 365, 181,…\n$ number_of_reviews_ltm          <dbl> 0, 0, 0, 0, 1, 0, 0, 3, 2, 0, 1, 0, 0, …\n$ license                        <chr> NA, NA, NA, NA, \"S0399\", NA, NA, \"S0399…\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\n#buffering\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\n\nsum(buffer_cycling$AREA)\n\n1556978 [m^2]\n\n\n#Point-in-polygon count\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    3.00    5.96    9.00   58.00 \n\n\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           58\n\n\n#Calculate the density of pre-school by planning subzone\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n#Explorotary Data Analysis(EDA)\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n#Using ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "In-class_Ex/data/Geospatial/stores.html",
    "href": "In-class_Ex/data/Geospatial/stores.html",
    "title": "IS415-Harith",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/data/Geospatial/study_area.html",
    "href": "In-class_Ex/data/Geospatial/study_area.html",
    "title": "IS415-Harith",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate geospatial data wrangling methods to prepare the data for water point mapping study. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Note that there are three Projected Coordinate Systems of Nigeria, they are: EPSG: 26391, 26392, and 26303. You can use any one of them.\nUsing appropriate tidyr and dplyr methods, derive the proportion of functional and non-functional water point at LGA level.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nVisualising the distribution of water point by using appropriate analytical visualisation methods."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#installing-and-loading-packages",
    "href": "In-class_Ex/In-class_Ex02.html#installing-and-loading-packages",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2.1 Installing and Loading Packages",
    "text": "2.1 Installing and Loading Packages\nFirstly, the code below will check if pacman has been installed. If it has not been installed, R will download and install it, before activating it for use during this session.\n\nif (!require('pacman', character.only = T)){\n  install.packages('pacman')\n}\nlibrary('pacman')\n\nNext, pacman assists us by helping us load R packages that we require, sf, tidyverse and funModeling.\n\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#geoboundaries-nigeria-level-2-administrative-boundary-dataset",
    "href": "In-class_Ex/In-class_Ex02.html#geoboundaries-nigeria-level-2-administrative-boundary-dataset",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2.2 geoBoundaries Nigeria Level-2 Administrative Boundary Dataset",
    "text": "2.2 geoBoundaries Nigeria Level-2 Administrative Boundary Dataset\n\n2.2.1 Importing geoBoundaries Nigeria Level-2 Administrative Boundary Dataset\nIn the code below, dsn specifies the filepath where the dataset is located and layer provides the filename of the dataset excluding the file extension.\n\ngbnigeria = st_read(dsn = \"data/Geospatial\", layer = \"geoBoundaries-NGA-ADM2\")\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the above message, it tells us that the dataset contains multipolygon features, containing 774 multipolygon features and 5 fields in the gbnigeria simple feature data frame and is in the WGS84 geographic coordinates system.\nLet us check the other dataset from Humanitarian data exchange.\n\nnigeria = st_read(dsn = \"data/Geospatial\", layer = \"nga_admbnda_adm2_osgof_20190417\")\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the above message, it tells us that the dataset contains multipolygon features, containing 774 multipolygon features and 16 fields in the gbnigeria simple feature data frame and is in the WGS84 geographic coordinates system.\nBy comparing both datasets, the dataset from Humanitarian Data Exchange is more favourable as we can tell which state the LGA area belongs too which will be beneficial for our analysis\n\n\n2.2.2 Checking the Coordinate Reference System\nIn the code below, we will check if the Coordinate Reference System has been specified correctly.\n\nst_crs(nigeria)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nAs seen above, the file has been configured correctly, having a WGS84 Geographic Coordinate System which maps to EPSG:4326.\n\n\n2.2.3 Converting the Coordinate Reference System\nIn the code below, we will convert the Geographic Coordinate Reference System from WGS84 to EPSG:26391 Projected Coordinate System.\n\nnigeria26391 <- st_transform(nigeria, crs = 26391)\n\n\nst_crs(nigeria26391)\n\nCoordinate Reference System:\n  User input: EPSG:26391 \n  wkt:\nPROJCRS[\"Minna / Nigeria West Belt\",\n    BASEGEOGCRS[\"Minna\",\n        DATUM[\"Minna\",\n            ELLIPSOID[\"Clarke 1880 (RGS)\",6378249.145,293.465,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4263]],\n    CONVERSION[\"Nigeria West Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",4,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",4.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.99975,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",230738.26,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Nigeria - onshore west of 6°30'E, onshore and offshore shelf.\"],\n        BBOX[3.57,2.69,13.9,6.5]],\n    ID[\"EPSG\",26391]]\n\n\nAfter running the code, we can confirm that the data frame has been converted to EPSG:26391 Projected Coordinate System."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#wpdx-aspatial-data",
    "href": "In-class_Ex/In-class_Ex02.html#wpdx-aspatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2.3 WPdx + Aspatial Data",
    "text": "2.3 WPdx + Aspatial Data\n\n2.3.1 Importing WPdx + Aspatial Data\nSince WPdx+ data set is in csv format, we will use read_csv() of readr package to import Water_Point_Data_Exchange_-_PlusWPdx.csv and output it to an R object called wpdx.\n\nwpdx <- read_csv(\"data/Aspatial/Water_Point_Data_Exchange.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\nlist(wpdx)\n\n[[1]]\n# A tibble: 95,008 × 70\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 61 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nOur output shows our wpdx tibble data frame consists of 97,478 rows and 74 columns. The useful fields we would be paying attention to is the #lat_deg and #lon_deg columns, which are in the decimal degree format. By viewing the Data Standard on wpdx’s website, we know that the latitude and longitude is in the wgs84 Geographic Coordinate System.\n\n\n2.3.2 Creating a Simple Feature Data Frame from an Aspatial Data Frame\nAs the geometry is available in wkt in the column New Georeferenced Column, we can use st_as_sfc() to import the geomtry\n\nwpdx$Geometry <- st_as_sfc(wpdx$`New Georeferenced Column`)\n\nAs there is no spatial data information, firstly, we assign the original projection when converting the tibble dataframe to sf. The original is wgs84 which is EPSG:4326.\n\nwpdx_sf <- st_sf(wpdx, crs=4326)\nwpdx_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNext, we then convert the projection to the appropriate decimal based projection system.\n\nwpdx_sf <- wpdx_sf %>%\n  st_transform(crs = 26391)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#excluding-redundant-fields",
    "href": "In-class_Ex/In-class_Ex02.html#excluding-redundant-fields",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3.1 Excluding Redundant Fields",
    "text": "3.1 Excluding Redundant Fields\nAs the wpdx sf dataframe consist of many redundant field, we use select() to select the fields which we want to retain.\n\nnigeria26391 <- nigeria26391 %>%\n  select(c(3:4, 8:9))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#checking-for-duplicate-name",
    "href": "In-class_Ex/In-class_Ex02.html#checking-for-duplicate-name",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3.2 Checking for Duplicate Name",
    "text": "3.2 Checking for Duplicate Name\nIt is important to check for duplicate name in the data main data fields. Using duplicated(), we can flag out LGA names that might be duplicated as shown below:\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nTo reduce duplication of LGA names, we will put the state names behind to make it more specific.\n\nnigeria26391$ADM2_EN[94] <- \"Bassa, Kogi\"\nnigeria26391$ADM2_EN[95] <- \"Bassa, Plateau\"\nnigeria26391$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nnigeria26391$ADM2_EN[305] <- \"Ifelodun, Osun\"\nnigeria26391$ADM2_EN[355] <- \"Irepodun, Kwara\"\nnigeria26391$ADM2_EN[356] <- \"Ireopodun, Osun\"\nnigeria26391$ADM2_EN[519] <- \"Nasarawa, Kano\"\nnigeria26391$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nnigeria26391$ADM2_EN[546] <- \"Obi, Benue\"\nnigeria26391$ADM2_EN[547] <- \"Obi, Nasarawa\"\nnigeria26391$ADM2_EN[693] <- \"Surulere, Lagos\"\nnigeria26391$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nLet us check now if the duplication has been resolved.\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#understanding-field-names",
    "href": "In-class_Ex/In-class_Ex02.html#understanding-field-names",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.1 Understanding Field Names",
    "text": "4.1 Understanding Field Names\nFirst, let us have a look at the #status_clean column which stores the information about Functional and Non-Functional data points. The code below returns all values that were used in the column.\n\nfreq(data = wpdx_sf,\n     input = '#status_clean')\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\nAs there might be issues performing mathematical calculations with NA labels, we will rename them to unknown.\nThe code below renames the column #status_clean to status_clean, select only the status_clean for manipulation and then replace all na values to unknown.\n\nwpdx_sf_nga <- wpdx_sf %>%\n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(status_clean, \"unknown\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#filtering-data",
    "href": "In-class_Ex/In-class_Ex02.html#filtering-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.2 Filtering Data",
    "text": "4.2 Filtering Data\nWith our previous knowledge, we can filter the data to obtain functional proportion counts in each LGA level. We will filter the wpdx_sf_nga dataframes to option functional and non-functional water points.\n\nwpdx_func <- wpdx_sf_nga %>% \n  filter(status_clean %in% \n           c(\"Functional\", \n             \"Functional but not in use\", \n             \"Functional but needs repair\"))\nwpdx_nonfunc <- wpdx_sf_nga %>% \n  filter(status_clean %in%\n          c(\"Abadoned/Decommissioned\", \n            \"Abandoned\",\n            \"Non-Functional due to dry season\",\n            \"Non-Functional\",\n            \"Non functional due to dry season\"))\nwpdx_unknown <- wpdx_sf_nga %>%\n  filter(status_clean == \"unknown\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#point-in-polygon-count",
    "href": "In-class_Ex/In-class_Ex02.html#point-in-polygon-count",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.3 Point-in-polygon Count",
    "text": "4.3 Point-in-polygon Count\nUtilising st_intersects() of sf package and lengths, we check where each data point for the water point which fall inside each LGA. We do each calculation separation so we can cross check later to ensure all the values sum to the same total.\n\nnigeria_wp <- nigeria26391 %>%\n  mutate(`total_wp` = lengths(\n    st_intersects(nigeria26391, wpdx_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(nigeria26391, wpdx_func))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(nigeria26391, wpdx_nonfunc))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(nigeria26391, wpdx_unknown)))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#saving-the-analytical-data-in-rds-format",
    "href": "In-class_Ex/In-class_Ex02.html#saving-the-analytical-data-in-rds-format",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.4 Saving the Analytical Data in rds format",
    "text": "4.4 Saving the Analytical Data in rds format\nIn order to retain the sf data structure for subsequent analysis, we should save the sf dataframe into rds format.\n\nwrite_rds(nigeria_wp, \"Data/rds/nigeria_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#plotting-the-distribution-of-total-water-points-by-lga-in-histogram",
    "href": "In-class_Ex/In-class_Ex02.html#plotting-the-distribution-of-total-water-points-by-lga-in-histogram",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.5 Plotting the Distribution of Total Water Points by LGA in Histogram",
    "text": "4.5 Plotting the Distribution of Total Water Points by LGA in Histogram\nNext, we will use mutate() of dplyr package to compute the proportion of Functional and Non- water points.\nThis is given by Functional Proportion = Functional Count / Total Count.\n\nggplot(data = nigeria_wp,\n       aes(x = total_wp)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\") +\n  geom_vline(aes(xintercept = mean(\n    total_wp, na.rm = T)),\n    color = \"red\",\n    linetype = \"dashed\",\n    size = 0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No of\\nLGAs\") +\n  theme(axis.title.y = element_text(angle = 0))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03.html",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps. For the purpose of this exercise, Nigeria water point data prepared during In-class Exercise 2 will be used.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#installing-and-loading-packages",
    "href": "In-class_Ex/In-class_Ex03.html#installing-and-loading-packages",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2.1 Installing and Loading Packages",
    "text": "2.1 Installing and Loading Packages\nFirstly, the code below will check if pacman has been installed. If it has not been installed, R will download and install it, before activating it for use during this session.\n\nif (!require('pacman', character.only = T)){\n  install.packages('pacman')\n}\nlibrary('pacman')\n\nNext, pacman assists us by helping us load R packages that we require, sf, tidyverse and tmap.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#importing-data",
    "href": "In-class_Ex/In-class_Ex03.html#importing-data",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nWe want to import the sf dataframe we have cleaned and prepared earlier in class exercise 02.\n\nNGA_wp <- read_rds(\"data/rds/nigeria_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#visualising-distribution-of-non-functional-water-points",
    "href": "In-class_Ex/In-class_Ex03.html#visualising-distribution-of-non-functional-water-points",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2.3 Visualising Distribution of Non-Functional Water Points",
    "text": "2.3 Visualising Distribution of Non-Functional Water Points\nHere, we will plot 2 maps, p1 which shows the functional water points and p2 by total number of water points for side-by-side visualization.\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGA\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGA\",\n            legend.outside = FALSE)\n\nUsing the tmap_arrange() function, we can arrange the two maps plotted on a single row for side-by-side comparison.\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "href": "In-class_Ex/In-class_Ex03.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points",
    "text": "3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWith the code below, we use mutate() to calculate the percentages of functional and nonpfunctional water points.\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional / total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional / total_wp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#plotting-map-of-rate",
    "href": "In-class_Ex/In-class_Ex03.html#plotting-map-of-rate",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3.2 Plotting Map of Rate",
    "text": "3.2 Plotting Map of Rate\nUtilising tmap, we can specify the NGA_wp dataframe to colour by pct_functional water points.\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water points by LGA\",\n            legend.outside = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#percentile-map",
    "href": "In-class_Ex/In-class_Ex03.html#percentile-map",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "4.1 Percentile Map",
    "text": "4.1 Percentile Map\nA percentile is a special type of quantile map with the following categories:\n\n0-1%\n1-10%\n10-50%\n50-90%\n90-99%\n99 - 100%\n\nTo create the map, we can set the breakpoints as c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1). Note that the start and endpoints needs to be included.\n\n4.1.1 Data Preparation\nFirstly, we exclude records with NA using the code below:\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nSecondly, we create a customised classification andextract the values.\n\npercent <- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n4.1.2 Function to get variable dataframe\nWith the function below, we can extract a variable out of the sf dataframe as a vector.\n\nget.var <- function(vname, df) {\n  v <- df[vname] %>%\n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n4.1.3 Percentile Mapping Function\nThis percentmap function allows us to take various inputs and automatically calculate the values and points needed for the percentile map.\nThe use of functions allows us to easily plot percentile maps of other variables flexibly without rewriting the entire code.\n\npercentmap <- function(vnam, df, legtitle = NA, mtitle = \"Percentile Map\"){\n  percent <- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n    tm_fill(vnam,\n            title = legtitle,\n            breaks = bperc,\n            palette = \"Blues\",\n            labels = c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"99% - 100%\")) +\n  tm_borders() +\n  tm_layout(main.title = mtitle,\n            title.position = c(\"right\", \"bottom\"))\n  }\n\nPlotting the Percentile Map of functional water points.\n\npercentmap(\"wp_functional\", NGA_wp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(maptools, sf, raster, spatstat, tmap)\n\nThings to learn from this code chunk.\nImporting spatial data\n\nchildcare_sf <- st_read(\"data/Geospatial/child-care-services-geojson.geojson\") %>% \n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf <- st_read(dsn = \"data/Geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data/Geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots(alph =0.5,\n          size =0.01) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "In-class_Ex/In-class_Ex04.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "In-class Exercise 4",
    "section": "4.5.2 Converting the Spatial* class into generic sp format",
    "text": "4.5.2 Converting the Spatial* class into generic sp format\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "In-class_Ex/In-class_Ex04.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "In-class Exercise 4",
    "section": "4.5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "4.5.3 Converting the generic sp format into spatstat’s ppp format\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nplot(childcare_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "pacman::p_load(tidyverse, tmap, sf, sfdep)\n\nImporting data shapefile for study area\n\nstudyArea <- st_read(dsn = \"data/Geospatial\", \n                layer = \"study_area\") %>%\n  st_transform(crs = 3829)\n\nReading layer `study_area' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 7 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 121.4836 ymin: 25.00776 xmax: 121.592 ymax: 25.09288\nGeodetic CRS:  TWD97\n\n\nImporting data shapefile for stores\n\nstores <- st_read(dsn = \"data/Geospatial\", \n                layer = \"stores\") %>%\n  st_transform(crs = 3829)\n\nReading layer `stores' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 1409 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 121.4902 ymin: 25.01257 xmax: 121.5874 ymax: 25.08557\nGeodetic CRS:  TWD97"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#visualizing-the-sf-layer",
    "href": "In-class_Ex/In-class_Ex05.html#visualizing-the-sf-layer",
    "title": "In-class_Ex05",
    "section": "Visualizing the sf layer",
    "text": "Visualizing the sf layer\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\n  tm_shape(stores) +\n  tm_dots(col = \"Name\", \n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12, 16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#local-colocation-quotients-lclq",
    "href": "In-class_Ex/In-class_Ex05.html#local-colocation-quotients-lclq",
    "title": "In-class_Ex05",
    "section": "Local Colocation Quotients (LCLQ)",
    "text": "Local Colocation Quotients (LCLQ)\n\nnb <- include_self(\n  st_knn(st_geometry(stores), 6))\n\n\nwt <- st_kernel_weights(nb,\n                        stores, \n                        \"gaussian\",\n                        adaptive = TRUE)\n\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\nA <- FamilyMart$Name\n\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\n\nLCLQ <- local_colocation(A, B, nb, wt , 49)\n\n\nLCLQ_stores <- cbind(stores, LCLQ)\n\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\n  tm_shape(LCLQ_stores) +\n  tm_dots(col = \"X7.Eleven\", \n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12, 16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06.html",
    "title": "In-class_Ex06",
    "section": "",
    "text": "Before we get started, we need to ensure that sfdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#the-data",
    "href": "In-class_Ex/In-class_Ex06.html#the-data",
    "title": "In-class_Ex06",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a goaspatial data set in ESRI shapefile format, and\nHunan_2021, an attribute data set in csv format.\n\n\nImporting geospatial data\n\nhunan <- st_read(dsn = \"data/Geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImport csv file into r environment\n\nhunan2012 <- read_csv(\"data/Aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#performing-relational-join",
    "href": "In-class_Ex/In-class_Ex06.html#performing-relational-join",
    "title": "In-class_Ex06",
    "section": "Performing relational join",
    "text": "Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n    tm_layout(main.title = 'Distribution of GDP per capita by distribution',\n    main.title.position = \"center\",\n    main.title.size = 1.2,\n    legend.height = 0.45,\n    legend.width = 0.35,\n    frame = TRUE) +\ntm_borders(alpha = 0.5) +\ntm_compass(type=\"8star\", size = 2) +\ntm_scale_bar() +\ntm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#visualising-regional-development-indicator",
    "href": "In-class_Ex/In-class_Ex06.html#visualising-regional-development-indicator",
    "title": "In-class_Ex06",
    "section": "Visualising Regional Development Indicator",
    "text": "Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap <- tm_shape(hunan_GDPPC) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc <- qtm(hunan_GDPPC, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#contingency-neighbours-method",
    "href": "In-class_Ex/In-class_Ex06.html#contingency-neighbours-method",
    "title": "In-class_Ex06",
    "section": "Contingency neighbours method",
    "text": "Contingency neighbours method\nIn the code chunk below, st_contiguity() is used to derive a continguity neighbour list by using queen’s method.\n\ncn_queen <-hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\nderive a contiguity neighbour list Using Rook’s method\n\ncn_rook <- hunan_GDPPC %>%\n  mutate(nb =st_contiguity(geometry),\n         queen =FALSE,\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#importing-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex07.html#importing-the-geospatial-data",
    "title": "In-class Exercise 7",
    "section": "1.1 Importing the Geospatial Data",
    "text": "1.1 Importing the Geospatial Data\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan <- st_read(dsn = \"Data/Geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#importing-csv-file-into-environment",
    "href": "In-class_Ex/In-class_Ex07.html#importing-csv-file-into-environment",
    "title": "In-class Exercise 7",
    "section": "1.2 Importing CSV file into environment",
    "text": "1.2 Importing CSV file into environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 <- read_csv(\"Data/Aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-relational-join",
    "href": "In-class_Ex/In-class_Ex07.html#performing-relational-join",
    "title": "In-class Exercise 7",
    "section": "1.3 Performing relational join",
    "text": "1.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\nIn order to retain the geospatial properties, the left data frame must be the sf data.frame (i.e. hunan)\n\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_Ex07.html#contiguity-weights-queens-method",
    "title": "In-class Exercise 7",
    "section": "3.1 Contiguity weights: Queen’s method",
    "text": "3.1 Contiguity weights: Queen’s method\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#contiguity-weights-rooks-method",
    "href": "In-class_Ex/In-class_Ex07.html#contiguity-weights-rooks-method",
    "title": "In-class Exercise 7",
    "section": "3.2 Contiguity weights: Rook’s method",
    "text": "3.2 Contiguity weights: Rook’s method\n\nwm_r <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n  queen = FALSE,\n  wt = st_weights(nb,\n                  style = \"W\"),\n  .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#computing-global-moran-i",
    "href": "In-class_Ex/In-class_Ex07.html#computing-global-moran-i",
    "title": "In-class Exercise 7",
    "section": "3.3 Computing Global Moran I",
    "text": "3.3 Computing Global Moran I\n\nmoranI <- global_moran(wm_q$GDPPC,\n                        wm_q$nb,\n                        wm_q$wt)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-global-moran-is-test",
    "href": "In-class_Ex/In-class_Ex07.html#performing-global-moran-is-test",
    "title": "In-class Exercise 7",
    "section": "3.4 Performing Global Moran I’s Test",
    "text": "3.4 Performing Global Moran I’s Test\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-global-morans-i-permutation-test",
    "href": "In-class_Ex/In-class_Ex07.html#performing-global-morans-i-permutation-test",
    "title": "In-class Exercise 7",
    "section": "3.5 Performing Global Moran’s I permutation test",
    "text": "3.5 Performing Global Moran’s I permutation test\nTo ensure results stay the same when rendering every time\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nIf observation values are small, it is better to use a higher number of simulations"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex07.html#computing-local-morans-i",
    "title": "In-class Exercise 7",
    "section": "3.6 Computing local Moran’s I",
    "text": "3.6 Computing local Moran’s I\n\nlisa <- wm_q %>%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %>%\n  unnest(local_moran)\n\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii   var_ii    z_ii    p_ii p_ii_…¹ p_fol…² skewn…³ kurtosis\n      <dbl>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 -0.00147  0.00177    4.18e-4 -0.158  0.874      0.82    0.41  -0.812  0.652  \n 2  0.0259   0.00641    1.05e-2  0.190  0.849      0.96    0.48  -1.09   1.89   \n 3 -0.0120  -0.0374     1.02e-1  0.0796 0.937      0.76    0.38   0.824  0.0461 \n 4  0.00102 -0.0000349  4.37e-6  0.506  0.613      0.64    0.32   1.04   1.61   \n 5  0.0148  -0.00340    1.65e-3  0.449  0.654      0.5     0.25   1.64   3.96   \n 6 -0.0388  -0.00339    5.45e-3 -0.480  0.631      0.82    0.41   0.614 -0.264  \n 7  3.37    -0.198      1.41e+0  3.00   0.00266    0.08    0.04   1.46   2.74   \n 8  1.56    -0.265      8.04e-1  2.04   0.0417     0.08    0.04   0.459 -0.519  \n 9  4.42     0.0450     1.79e+0  3.27   0.00108    0.02    0.01   0.746 -0.00582\n10 -0.399   -0.0505     8.59e-2 -1.19   0.234      0.28    0.14  -0.685  0.134  \n# … with 78 more rows, 12 more variables: mean <fct>, median <fct>,\n#   pysal <fct>, nb <nb>, wt <list>, NAME_2 <chr>, ID_3 <int>, NAME_3 <chr>,\n#   ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>, geometry <POLYGON [°]>, and\n#   abbreviated variable names ¹​p_ii_sim, ²​p_folded_sim, ³​skewness\n\n\nVariable mean and pysal value should be the same. For take home exercise 2, stay with mean"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-local-morans-i",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-local-morans-i",
    "title": "In-class Exercise 7",
    "section": "3.7 Visualising local Moran’s I",
    "text": "3.7 Visualising local Moran’s I\n\n3.7.1 Computing ii\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n3.7.2 Computing p_ii\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIdeally should use p_ii_sim variable of lisa so that results produced is stable.\n\n\n3.7.3 Visualising the local Moran’s I Map\n\nlisa_sig <- lisa %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nFor take home exercise 2, add on to use insignificant on top of LL,HL,LH,HH, no need to use LISA but hot & cold spot areas"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-gi",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-gi",
    "title": "In-class Exercise 7",
    "section": "4.1 Visualising Gi*",
    "text": "4.1 Visualising Gi*\n\ntmap_mode(\"view\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-the-p-value-of-hcsa",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-the-p-value-of-hcsa",
    "title": "In-class Exercise 7",
    "section": "4.2 Visualising the p value of HCSA",
    "text": "4.2 Visualising the p value of HCSA\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#import-files-of-hunan-gdppc",
    "href": "In-class_Ex/In-class_Ex07.html#import-files-of-hunan-gdppc",
    "title": "In-class Exercise 7",
    "section": "5.1 Import files of Hunan GDPPC",
    "text": "5.1 Import files of Hunan GDPPC\n\nGDPPC <- read_csv(\"Data/Aspatial/Hunan_GDPPC.csv\")\n\n\n5.1.1 Creating a time series cube\n\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\nTo construct spacetime cube, we must obtain the location and time\n\nGDPPC_nb <- GDPPC_st %>%\n  activate(\"geometry\") %>%\n  mutate(\n    nb = include_self(st_contiguity(geometry)),\n    wt = st_weights(nb)\n  ) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#arrange-to-show-significant-hotspot-and-coldspot-areas",
    "href": "In-class_Ex/In-class_Ex07.html#arrange-to-show-significant-hotspot-and-coldspot-areas",
    "title": "In-class Exercise 7",
    "section": "5.2 Arrange to show significant hotspot and coldspot areas",
    "text": "5.2 Arrange to show significant hotspot and coldspot areas"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex07.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 7",
    "section": "5.3 Performing Emerging Hotspot Analysis",
    "text": "5.3 Performing Emerging Hotspot Analysis\n\nehsa <- emerging_hotspot_analysis(\n  x = GDPPC_st,\n  .var = \"GDPPC\",\n  k = 1,\n  nsim = 99\n)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-ehsa",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-ehsa",
    "title": "In-class Exercise 7",
    "section": "5.4 Visualising EHSA",
    "text": "5.4 Visualising EHSA"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-Harith",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications\nThis is the course website of IS415 I study this term. You will find my coursework on this website."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "This study aims to analyse the geographical distribution of functional and non-function water points and their co-locations if any in Osun State, Nigeria by applying appropriate spatial point patterns analysis methods.\n\nTo address the issue of providing a clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point-related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#data",
    "title": "Take-home Exercise 1",
    "section": "2 Data",
    "text": "2 Data\nThe following data sets will be used in the analysis.\nAspatial Data:\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\nGeospatial Data:\nThis study will focus of Osun State, Nigeria. The state boundary GIS data of Nigeria can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n\n\n\n\n\n\nData\nFormat\nDescription\nSource\n\n\n\n\nNigeria Level-2 Administrative Boundary\nShapefile\n\nHumanitarian Data Exchange(data.humdata.org)\nor\ngeoBoundaries (geoboundaries.org)\n\n\nWPdx Global Data Repositories (WPdx+)\nCSV\n\nwaterpointdata.org"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#install-and-load-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#install-and-load-packages",
    "title": "Take-home Exercise 1",
    "section": "3 Install and load packages",
    "text": "3 Install and load packages\nFirstly, the code below will check if pacman has been installed. If it has not been installed, R will download and install it, before activating it for use during this session.\n\nif (!require('pacman', character.only = T)){\n  install.packages('pacman')\n}\nlibrary('pacman')\n\nTo get started, the following packages will be used for this exercise:\ntidyverse, funModeling, maptools, sf, raster, spatstat & tmap.\n\npacman::p_load(maptools, sf, tidyverse, raster, spatstat, tmap, sfdep, funModeling)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#importing-geoboundaries-nigeria-level-2-administrative-boundary-dataset",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#importing-geoboundaries-nigeria-level-2-administrative-boundary-dataset",
    "title": "Take-home Exercise 1",
    "section": "4.1 Importing geoBoundaries Nigeria Level-2 Administrative Boundary Dataset",
    "text": "4.1 Importing geoBoundaries Nigeria Level-2 Administrative Boundary Dataset\n\nnigeria = st_read(dsn = \"data/Geospatial\", layer = \"nga_admbnda_adm2_osgof_20190417\")\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\Harith-oh\\IS415-Harith\\Take-home_Ex\\Take-home_Ex01\\data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nCheck CRS\n\nst_crs(nigeria)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#converting-the-coordinating-reference-system",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#converting-the-coordinating-reference-system",
    "title": "Take-home Exercise 1",
    "section": "4.2 Converting the Coordinating Reference System",
    "text": "4.2 Converting the Coordinating Reference System\nIn the code below, we will convert the Geographic Coordinate Reference System from WGS84 to EPSG:26391 Projected Coordinate System.\n\nnigeria26391 <- st_transform(nigeria, crs = 26391)\n\n\nst_crs(nigeria26391)\n\nCoordinate Reference System:\n  User input: EPSG:26391 \n  wkt:\nPROJCRS[\"Minna / Nigeria West Belt\",\n    BASEGEOGCRS[\"Minna\",\n        DATUM[\"Minna\",\n            ELLIPSOID[\"Clarke 1880 (RGS)\",6378249.145,293.465,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4263]],\n    CONVERSION[\"Nigeria West Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",4,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",4.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.99975,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",230738.26,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Nigeria - onshore west of 6°30'E, onshore and offshore shelf.\"],\n        BBOX[3.57,2.69,13.9,6.5]],\n    ID[\"EPSG\",26391]]\n\n\nAfter running the code, we can confirm that the data frame has been converted to EPSG:26391 Projected Coordinate System."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#importing-wpdx-aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#importing-wpdx-aspatial-data",
    "title": "Take-home Exercise 1",
    "section": "4.3 Importing WPdx + Aspatial Data",
    "text": "4.3 Importing WPdx + Aspatial Data\nSince WPdx+ data set is in csv format, we will use read_csv() of readr package to import Water_Point_Data_Exchange_-_PlusWPdx.csv and output it to an R object called wpdx.\n\nwpdx <- read_csv(\"data/Aspatial/Water_Point_Data_Exchange.csv\") %>% filter(`#clean_country_name` == \"Nigeria\")\n\n\nlist(wpdx)\n\n[[1]]\n# A tibble: 95,008 × 70\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 61 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nOur output shows our wpdx tibble data frame consists of 97,478 rows and 74 columns. The useful fields we would be paying attention to is the #lat_deg and #lon_deg columns, which are in the decimal degree format. By viewing the Data Standard on wpdx’s website, we know that the latitude and longitude is in the wgs84 Geographic Coordinate System.\n\n4.3.1 Creating a simple data frame from an Aspatial Data Frame\nAs the geometry is available in wkt in the column New Georeferenced Column, we can use st_as_sfc() to import the geomtry\n\nwpdx$Geometry <- st_as_sfc(wpdx$`New Georeferenced Column`)\n\nAs there is no spatial data information, firstly, we assign the original projection when converting the tibble dataframe to sf. The original is wgs84 which is EPSG:4326.\n\nwpdx_sf <- st_sf(wpdx, crs=4326)\nwpdx_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNext, we then convert the projection to the appropriate decimal based projection system.\n\nwpdx_sf <- wpdx_sf %>%\n  st_transform(crs = 26391)\n\n\nwpdx_spdf <- as_Spatial(wpdx_sf)\nwpdx_spdf\n\nclass       : SpatialPointsDataFrame \nfeatures    : 95008 \nextent      : 32536.82, 1292096, 33461.24, 1091052  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=4.5 +k=0.99975 +x_0=230738.26 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 70\nnames       : row_id,                                     X.source, X.lat_deg,  X.lon_deg,          X.report_date, X.status_id, X.water_source_clean, X.water_source_category, X.water_tech_clean, X.water_tech_category, X.facility_type, X.clean_country_name, X.clean_adm1, X.clean_adm2, X.clean_adm3, ... \nmin values  :  10732, Federal Ministry of Water Resources, Nigeria, 4.3018117,   2.707441, 01/01/2010 12:00:00 AM,          No,             Borehole,             Piped Water,          Hand Pump,             Hand Pump,        Improved,              Nigeria,         Abia,    Aba North,           NA, ... \nmax values  : 681838,                                  WaterAid UK, 13.865675, 14.2182849, 12/31/2014 12:00:00 AM,         Yes,       Protected Well,                    Well,           Tapstand,              Tapstand,        Improved,              Nigeria,      Zamfara,         Zuru,           NA, ..."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#geospatial-data-cleaning",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#geospatial-data-cleaning",
    "title": "Take-home Exercise 1",
    "section": "5.1 Geospatial Data Cleaning",
    "text": "5.1 Geospatial Data Cleaning\nAs the wpdx sf dataframe consist of many redundant field, we use select() to select the fields which we want to retain.\n\nnigeria26391 <- nigeria26391 %>%\n  dplyr::select(c(3:4, 8:9))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#checking-for-duplicate-name",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#checking-for-duplicate-name",
    "title": "Take-home Exercise 1",
    "section": "5.2 Checking for Duplicate Name",
    "text": "5.2 Checking for Duplicate Name\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nTo reduce duplication of LGA names, we will put the state names behind to make it more specific.\n\nnigeria26391$ADM2_EN[94] <- \"Bassa, Kogi\"\nnigeria26391$ADM2_EN[95] <- \"Bassa, Plateau\"\nnigeria26391$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nnigeria26391$ADM2_EN[305] <- \"Ifelodun, Osun\"\nnigeria26391$ADM2_EN[355] <- \"Irepodun, Kwara\"\nnigeria26391$ADM2_EN[356] <- \"Ireopodun, Osun\"\nnigeria26391$ADM2_EN[519] <- \"Nasarawa, Kano\"\nnigeria26391$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nnigeria26391$ADM2_EN[546] <- \"Obi, Benue\"\nnigeria26391$ADM2_EN[547] <- \"Obi, Nasarawa\"\nnigeria26391$ADM2_EN[693] <- \"Surulere, Lagos\"\nnigeria26391$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nTo check if the duplication has been resolved.\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#understanding-field-names",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#understanding-field-names",
    "title": "Take-home Exercise 1",
    "section": "6.1 Understanding Field Names",
    "text": "6.1 Understanding Field Names\nFirst, let us have a look at the #status_clean column which stores the information about Functional and Non-Functional data points. The code below returns all values that were used in the column.\n\nfreq(data = wpdx_sf,\n     input = '#status_clean')\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\nAs there might be issues performing mathematical calculations with NA labels, we will rename them to unknown.\nThe code below renames the column #status_clean to status_clean, select only the status_clean for manipulation and then replace all na values to unknown.\n\nwpdx_sf_nga <- wpdx_sf %>%\n  rename(status_clean = '#status_clean') %>%\n  dplyr::select(status_clean) %>%\n  mutate(status_clean = replace_na(status_clean, \"unknown\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#ing-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#ing-data",
    "title": "Take-home Exercise 1",
    "section": "6.2 ing Data",
    "text": "6.2 ing Data\nWith our previous knowledge, we can the data to obtain functional proportion counts in each LGA level. We will the wpdx_sf_nga dataframes to option functional and non-functional water points.\n\nwpdx_func <- wpdx_sf_nga %>% \n  filter(status_clean %in% \n           c(\"Functional\", \n             \"Functional but not in use\", \n             \"Functional but needs repair\"))\nwpdx_nonfunc <- wpdx_sf_nga %>% \n  filter(status_clean %in%\n          c(\"Abadoned/Decommissioned\", \n            \"Abandoned\",\n            \"Non-Functional due to dry season\",\n            \"Non-Functional\",\n            \"Non functional due to dry season\"))\nwpdx_unknown <- wpdx_sf_nga %>%\n  filter(status_clean == \"unknown\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#point-in-polygon-count",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#point-in-polygon-count",
    "title": "Take-home Exercise 1",
    "section": "6.3 Point-in-polygon Count",
    "text": "6.3 Point-in-polygon Count\nUtilising st_intersects() of sf package and lengths, we check where each data point for the water point which fall inside each LGA. We do each calculation separation so we can cross check later to ensure all the values sum to the same total.\n\nnigeria_wp <- nigeria26391 %>%\n  mutate(`total_wp` = lengths(\n    st_intersects(nigeria26391, wpdx_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(nigeria26391, wpdx_func))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(nigeria26391, wpdx_nonfunc))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(nigeria26391, wpdx_unknown)))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#saving-the-analytical-data-in-rds-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#saving-the-analytical-data-in-rds-format",
    "title": "Take-home Exercise 1",
    "section": "6.4 Saving the Analytical Data in rds format",
    "text": "6.4 Saving the Analytical Data in rds format\nIn order to retain the sf data structure for subsequent analysis, we should save the sf dataframe into rds format.\n\nwrite_rds(nigeria_wp, \"Data/rds/nigeria_wp.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#plotting-the-distribution-of-total-water-points-by-lga-in-histogram",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#plotting-the-distribution-of-total-water-points-by-lga-in-histogram",
    "title": "Take-home Exercise 1",
    "section": "6.5 Plotting the Distribution of Total Water Points by LGA in Histogram",
    "text": "6.5 Plotting the Distribution of Total Water Points by LGA in Histogram\nNext, we will use mutate() of dplyr package to compute the proportion of Functional and Non- water points.\nThis is given by Functional Proportion = Functional Count / Total Count.\n\nggplot(data = nigeria_wp,\n       aes(x = total_wp)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\") +\n  geom_vline(aes(xintercept = mean(\n    total_wp, na.rm = T)),\n    color = \"red\",\n    linetype = \"dashed\",\n    size = 0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No of\\nLGAs\") +\n  theme(axis.title.y = element_text(angle = 0))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#exploratory-spatial-data-analysis-esda",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#exploratory-spatial-data-analysis-esda",
    "title": "Take-home Exercise 1",
    "section": "7 Exploratory Spatial Data Analysis (ESDA)",
    "text": "7 Exploratory Spatial Data Analysis (ESDA)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#import-the-data-saved-in-rds",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#import-the-data-saved-in-rds",
    "title": "Take-home Exercise 1",
    "section": "7.1 Import the data saved in rds",
    "text": "7.1 Import the data saved in rds\nWe want to import the sf dataframe we have cleaned and prepared earlier\n\nNGA_wp <- read_rds(\"Data/rds/nigeria_wp.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#visualising-distribution-of-non-functional-water-points",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#visualising-distribution-of-non-functional-water-points",
    "title": "Take-home Exercise 1",
    "section": "7.2 Visualising Distribution of Non-Functional Water Points",
    "text": "7.2 Visualising Distribution of Non-Functional Water Points\nHere, we will plot 2 maps, p1 which shows the functional water points and p2 by total number of water points for side-by-side visualization.\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGA\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGA\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#visualising-the-sf-layers",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#visualising-the-sf-layers",
    "title": "Take-home Exercise 1",
    "section": "8 Visualising the sf layers",
    "text": "8 Visualising the sf layers\nIt is to ensure that they have been imported properly and been projected on an appropriate projection system.\n\ntmap_mode(\"view\")\ntm_shape(NGA_wp) +\n  tm_polygons() +\n  tm_dots()\n\n\n\n\n\n\n\nNGA <- as_Spatial(NGA_wp)\n\n\nwpdx_sp <- as(wpdx_spdf, \"SpatialPoints\")\nNGA_sp <- as(NGA, \"SpatialPolygons\")\n\n\nosun = NGA[NGA@data$ADM1_EN == \"Osun\",]\n\nTo plot\n\nplot(osun, main = \"Osun\")\n\n\n\n\nNext, conversion of SpatialPolygonsDataFrame layers into generic spatialpolygons layers\n\nosun_sp = as(osun, \"SpatialPolygons\")\n\nCreating owin object to convert the above spatialpolygons objects into owin objects that is required by spatstat\n\nosun_owin = as(osun_sp, \"owin\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Take-home Exercise 1",
    "section": "8.2 Converting the generic sp format into spatstat’s ppp format",
    "text": "8.2 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nwpdx_ppp <- as(wpdx_sp, \"ppp\")\nwpdx_ppp\n\nPlanar point pattern: 95008 points\nwindow: rectangle = [32536.8, 1292096.3] x [33461.2, 1091051.6] units\n\n\n\nsummary(wpdx_ppp)\n\nPlanar point pattern:  95008 points\nAverage intensity 7.132208e-08 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: rectangle = [32536.8, 1292096.3] x [33461.2, 1091051.6] units\n                    (1260000 x 1058000 units)\nWindow area = 1.3321e+12 square units\n\n\n\nany(duplicated(wpdx_ppp))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#combining-osun-water-points",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#combining-osun-water-points",
    "title": "Take-home Exercise 1",
    "section": "8.3 Combining osun water points",
    "text": "8.3 Combining osun water points\n\nnigeria_osun_PPP = wpdx_ppp[osun_owin]\n\nrescale() function is used to transform the unit of measurement from metre to kilometre\n\nnigeria_osun_PPP.km = rescale(nigeria_osun_PPP, 1000, \"km\")\n\nPlotting the Osun water point study area\n\nplot(nigeria_osun_PPP.km, main = \"Osun\")\n\n\n\n\nVisualising the SF layer for Osun\n\ntmap_mode(\"view\")\ntm_shape(osun) +\n  tm_polygons() +\n  tm_dots(size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#computing-kde-of-osun-state",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#computing-kde-of-osun-state",
    "title": "Take-home Exercise 1",
    "section": "9.1 Computing KDE of Osun State",
    "text": "9.1 Computing KDE of Osun State\nThe code below will be used to compute the KDE of Osun. bw.diggle method is used to derive the bandwidth\n\nplot(density(nigeria_osun_PPP.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Osun\")\n\n\n\n\nComputing fixed bandwidth KDE\n\nplot(density(nigeria_osun_PPP.km,\n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Osun\")\n\n\n\n\nThe advantage of kernel density map over point map is that it spreads the known quantity in color shades of waterpoints for each location in Osun State of Nigeria. Whereas for Kernel point, it only provides the pointer of waterpoints for each location in Osun State and does not state the number unless clicked.\n\nwpdx_ppp <- as(wpdx_sp, \"ppp\")\nwpdx_ppp\n\nPlanar point pattern: 95008 points\nwindow: rectangle = [32536.8, 1292096.3] x [33461.2, 1091051.6] units\n\n\n\nsummary(wpdx_ppp)\n\nPlanar point pattern:  95008 points\nAverage intensity 7.132208e-08 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: rectangle = [32536.8, 1292096.3] x [33461.2, 1091051.6] units\n                    (1260000 x 1058000 units)\nWindow area = 1.3321e+12 square units\n\n\n\nany(duplicated(wpdx_ppp))\n\n[1] FALSE\n\n\n\nosun1 <- tm_shape(osun) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGA\",\n            legend.outside = FALSE)\n\n\nosun2 <- tm_shape(osun) +\n  tm_fill(\"wp_nonfunctional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGA\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(osun2, osun1, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkde_osun_bw <- density(nigeria_osun_PPP,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\nplot(kde_osun_bw)\n\n\n\n\n\nosun_bw <- bw.diggle(nigeria_osun_PPP)\nosun_bw\n\n   sigma \n217.8511 \n\n\n\nosun_PPP.km <- rescale(nigeria_osun_PPP, 1000, \"km\")\n\n\nkde_osun_PPP.bw <- density(nigeria_osun_PPP.km, sigma1=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_osun_PPP.bw)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#second-spatial-point-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#second-spatial-point-analysis",
    "title": "Take-home Exercise 1",
    "section": "10 Second Spatial Point Analysis",
    "text": "10 Second Spatial Point Analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#performing-clarks-evans-test-of-osun-state",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#performing-clarks-evans-test-of-osun-state",
    "title": "Take-home Exercise 1",
    "section": "10.1 Performing Clarks Evans Test of Osun State",
    "text": "10.1 Performing Clarks Evans Test of Osun State\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of waterpoints are randomly distributed.\nH1= The distribution of waterpoints are not randomly distributed.\nThe 95% confidence interval will be used.\n\nclarkevans.test(nigeria_osun_PPP,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 999 simulations of CSR with fixed n\n\ndata:  nigeria_osun_PPP\nR = 0.42836, p-value = 0.002\nalternative hypothesis: two-sided\n\n\nGiven the p value of 0.002 which is lesser than 0.05 of 95% confidence interval, we can conclude that the null hypothesis (H0) should be rejected and the distribution of waterpoints are not randomly distributed."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#analysing-second-spatial-point-using-l-function",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#analysing-second-spatial-point-using-l-function",
    "title": "Take-home Exercise 1",
    "section": "10.2 Analysing Second Spatial Point using L function",
    "text": "10.2 Analysing Second Spatial Point using L function\nIn this section, second spatial point L-function will be using Lest() of spatstat package. Monta carlo simulation test will be used using envelope() of spatstat package."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#computing-l-function-estimation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#computing-l-function-estimation",
    "title": "Take-home Exercise 1",
    "section": "10.2.1 Computing L Function Estimation",
    "text": "10.2.1 Computing L Function Estimation\n\n#L_ck = Lest(nigeria_osun_PPP, correction = \"Ripley\")\n#plot(L_ck, . -r ~ r, \n    # ylab= \"L(d)-r\", xlab = \"d(m)\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#performing-complete-spatial-random-test",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#performing-complete-spatial-random-test",
    "title": "Take-home Exercise 1",
    "section": "10.2.2 Performing Complete Spatial Random Test",
    "text": "10.2.2 Performing Complete Spatial Random Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of Water Point at Osun State are randomly distributed.\nH1= The distribution of Water Point at Osun State are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing\n\n#L_ck.csr <- envelope(nigeria_osun_PPP, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\n#plot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\nAs shown in the chart, given the L_ck.csr plotted graph exited the envelope this shows that it is lesser than 0.05 of 95% confidence interval, H0 is rejected and we can conclude that the distribution of waterpoints are not randomly distributed."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#local-colocation-quotients-lclq",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#local-colocation-quotients-lclq",
    "title": "Take-home Exercise 1",
    "section": "11 Local Colocation Quotients (LCLQ)",
    "text": "11 Local Colocation Quotients (LCLQ)\nNote:\nThe following code chunks above are commented to allow the take_home_ex1.html file (over 70mb) to commit and push to my github repository. The images from section 11 onwards are statically pasted to reduce the file size.\nBefore we perform the local colocation quotients, We will the wpdx_sf_nga dataframes to include both functional and non-functional water points only.\n\n wpdx_lclq <- wpdx_sf_nga %>% \n   filter(status_clean %in% \n            c(\"Functional\",\n             \"Non-Functional\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#visualizing-the-sf-layer",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#visualizing-the-sf-layer",
    "title": "Take-home Exercise 1",
    "section": "11.1 Visualizing the sf layer",
    "text": "11.1 Visualizing the sf layer\nFirstly, we want to visualise and observe how the sf layer will look like with the water points in osun.\n\n#tmap_mode(\"view\")\n#tm_shape(osun) +\n  #tm_polygons() +\n  #tm_shape(wpdx_lclq) +\n  #tm_dots(col = \"status_clean\", \n          #size = 0.01,\n          #border.col = \"black\",\n          #border.lwd = 0.5)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#applying-local-colocation-quotients-lclq",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex1.html#applying-local-colocation-quotients-lclq",
    "title": "Take-home Exercise 1",
    "section": "11.2 Applying Local Colocation Quotients (LCLQ)",
    "text": "11.2 Applying Local Colocation Quotients (LCLQ)\nIn this section, we will perform the Local Colocation Quotients (LCLQ) to measure the concentration of the waterpoints in Osun by using local_colocation.\nAfter visualising the sf layers, it looks like most waterpoints in the Osun state are highly concentrated. Thus, the test hypotheses are:\nHo = The concentration of the waterpoints in the Osun generally low\nH1= The concentration of the waterpoints in the Osun state are generally high\nTo perform the Local Colocation Quotients (LCLQ), we will first identify the k nearest neighbors for given point geometry.\n\n#nb <- include_self(\n  #st_knn(st_geometry(wpdx_lclq), 6))\n\nAfter that, we will create a weights list using a kernel function.\n\n#wt <- st_kernel_weights(nb,\n                        #wpdx_lclq, \n                        #\"gaussian\",\n                        #adaptive = TRUE)\n\nWe will filter functional and non-functional from the wpdx_lclq whereby Functional will be A and Nonfunctional will be B.\n\n#Functional <- wpdx_lclq %>%\n  #(status_clean == \"Functional\")\n#A <- Functional$status_clean\n\n\n#NonFunctional <- wpdx_lclq %>%\n  #(status_clean == \"Non-Functional\")\n#B <- NonFunctional$status_clean\n\nWe will use the local_colocation() to calculate LCLQ.\n\n#LCLQ <- local_colocation(A, B, nb, wt , 20)\n\nAfter that, we will use the column bind function to merge two data frames together given that the number of rows in both the data frames are equal.\n\n#LCLQ_sf_nga <- cbind(wpdx_lclq, LCLQ)\n\nFinally, we will use the tmap() to visualise the results of LCLQ.\n\n#tmap_mode(\"view\")\n#tm_shape(osun) +\n  #tm_polygons() +\n  #tm_shape(LCLQ_sf_nga) +\n  #tm_dots(col = \"Non.Functional\", \n          #size = 0.01,\n          #border.col = \"black\",\n          #border.lwd = 0.5)\n\n\nFrom the visualisation above, we can conclude that there are many ‘missing’ water points in Osun which implies that the points on the upper side of Osun State are more concentrated. Thus, we will reject H1 for this case since the concentration are generally low.\nAfter performing the ‘clark evans test’, ‘L function’ and the ‘local colocation quotients’, we can conclude that H0 is rejected, the functional and non functional water points in Osun State are not randomly distributed with low concentration where the upper part of Osun state are slightly more concentrated.\nEnd of take home exercise 1"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html",
    "href": "Take-home_Ex2/Take_home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Since late December 2019, an outbreak of a novel coronavirus disease (COVID-19; previously known as 2019-nCoV) was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. In general, COVID-19 is an acute resolved disease but it can also be deadly, with a 2% case fatality rate.\nThe COVID-19 vaccination in Indonesia is an ongoing mass immunisation in response to the COVID-19 pandemic in Indonesia. On 13 January 2021, the program commenced when President Joko Widodo was vaccinated at the presidential palace. In terms of total doses given, Indonesia ranks third in Asia and fifth in the world.\nAccording to wikipedia, as of 5 February 2023 at 18:00 WIB (UTC+7), 204,266,655 people had received the first dose of the vaccine and 175,131,893 people had been fully vaccinated; 69,597,474 of them had been inoculated with the booster or the third dose, while 1,585,164 had received the fourth dose. Jakarta has the highest percentage of population fully vaccinated with 103.46%, followed by Bali and Special Region of Yogyakarta with 85.45% and 83.02% respectively.\nDespite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with relatively higher number of vaccination rate and how they changed over time.\n\n\n\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.\nf## The Task\nThe specific tasks of this take-home exercise are as follows:\n\n\n\nCompute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,\nPrepare the monthly vaccination rate maps by using appropriate tmap functions,\nDescribe the spatial patterns revealed by the choropleth maps (not more than 200 words).\n\n\n\n\nWith reference to the vaccination rate maps prepared in ESDA:\n\nCompute local Gi* values of the monthly vaccination rate,\nDisplay the Gi* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value < 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 250 words).\n\n\n\n\nWith reference to the local Gi* values of the vaccination rate maps prepared in the previous section:\n\nPerform Mann-Kendall Test by using the spatio-temporal local Gi* values,\nSelect three sub-districts and describe the temporal trends revealed (not more than 250 words), and\nPrepared a EHSA map of the Gi* values of vaccination rate. The maps should only display the significant (i.e. p-value < 0.05).\nWith reference to the EHSA map prepared, describe the spatial patterns revealed. (not more than 250 words).\n\n\n\n\n\n\n\nFor the purpose of this assignment, data from Riwayat File Vaksinasi DKI Jakarta will be used. Daily vaccination data are provides. You are only required to download either the first day of the month or last day of the month of the study period.\n\n\n\nFor the purpose of this study, DKI Jakarta administration boundary 2019 will be used. The data set can be downloaded at Indonesia Geospatial portal, specifically at this page.\nNote\n\nThe national Projected Coordinates Systems of Indonesia is DGN95 / Indonesia TM-3 zone 54.1.\nExclude all the outer islands from the DKI Jakarta sf data frame, and\nRetain the first nine fields in the DKI Jakarta sf data frame. The ninth field JUMLAH_PEN = Total Population.\nReference was taken from the senior sample submissions for the code for this section, with credit to Megan - https://is415-msty.netlify.app/posts/2021-09-10-take-home-exercise-1/"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#importing-the-geospatial-data",
    "href": "Take-home_Ex2/Take_home_Ex2.html#importing-the-geospatial-data",
    "title": "Take Home Exercise 2",
    "section": "1.1 Importing the Geospatial Data",
    "text": "1.1 Importing the Geospatial Data\nThe code chunk below uses st_read() of sf package to import BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA shapefile into R. The imported shapefile will be simple features Object of sf. As we can see, the assigned coordinates system is WGS 84, the ‘World Geodetic System 1984’. In the context of this dataset, this isn’t appropriate: as this is an Indonesian-specific geospatial dataset, we should be using the national CRS of Indonesia, DGN95, the ‘Datum Geodesi Nasional 1995’, ESPG code 23845. st_transform will be used to rectify the coordinate system\n\nbd_jakarta <- st_read(dsn = \"data/Geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\")\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\Harith-oh\\IS415-Harith\\Take-home_Ex2\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nFrom the output message we can see that there are 269 features and 161 fields. The assigned CRS is WGS 84, the ‘World Geodetic System 1984’. This is not right, and will be rectify that later."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#data-pre-processing",
    "href": "Take-home_Ex2/Take_home_Ex2.html#data-pre-processing",
    "title": "Take Home Exercise 2",
    "section": "1.2 Data Pre-Processing",
    "text": "1.2 Data Pre-Processing\n\n1.2.1 Check for Missing Values\nNow lets check if there are any missing values\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 2 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.8412 ymin: -6.154036 xmax: 106.8612 ymax: -6.144973\nGeodetic CRS:  WGS 84\n    OBJECT_ID KODE_DESA             DESA   KODE    PROVINSI KAB_KOTA KECAMATAN\n243     25645  31888888     DANAU SUNTER 318888 DKI JAKARTA     <NA>      <NA>\n244     25646  31888888 DANAU SUNTER DLL 318888 DKI JAKARTA     <NA>      <NA>\n    DESA_KELUR JUMLAH_PEN JUMLAH_KK LUAS_WILAY KEPADATAN PERPINDAHA JUMLAH_MEN\n243       <NA>          0         0          0         0          0          0\n244       <NA>          0         0          0         0          0          0\n    PERUBAHAN WAJIB_KTP SILAM KRISTEN KHATOLIK HINDU BUDHA KONGHUCU KEPERCAYAA\n243         0         0     0       0        0     0     0        0          0\n244         0         0     0       0        0     0     0        0          0\n    PRIA WANITA BELUM_KAWI KAWIN CERAI_HIDU CERAI_MATI U0 U5 U10 U15 U20 U25\n243    0      0          0     0          0          0  0  0   0   0   0   0\n244    0      0          0     0          0          0  0  0   0   0   0   0\n    U30 U35 U40 U45 U50 U55 U60 U65 U70 U75 TIDAK_BELU BELUM_TAMA TAMAT_SD SLTP\n243   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n244   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n    SLTA DIPLOMA_I DIPLOMA_II DIPLOMA_IV STRATA_II STRATA_III BELUM_TIDA\n243    0         0          0          0         0          0          0\n244    0         0          0          0         0          0          0\n    APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN NELAYAN AGAMA_DAN PELAJAR_MA\n243          0          0          0         0       0         0          0\n244          0          0          0         0       0         0          0\n    TENAGA_KES PENSIUNAN LAINNYA GENERATED KODE_DES_1 BELUM_ MENGUR_ PELAJAR_\n243          0         0       0      <NA>       <NA>      0       0        0\n244          0         0       0      <NA>       <NA>      0       0        0\n    PENSIUNA_1 PEGAWAI_ TENTARA KEPOLISIAN PERDAG_ PETANI PETERN_ NELAYAN_1\n243          0        0       0          0       0      0       0         0\n244          0        0       0          0       0      0       0         0\n    INDUSTR_ KONSTR_ TRANSP_ KARYAW_ KARYAW1 KARYAW1_1 KARYAW1_12 BURUH BURUH_\n243        0       0       0       0       0         0          0     0      0\n244        0       0       0       0       0         0          0     0      0\n    BURUH1 BURUH1_1 PEMBANT_ TUKANG TUKANG_1 TUKANG_12 TUKANG__13 TUKANG__14\n243      0        0        0      0        0         0          0          0\n244      0        0        0      0        0         0          0          0\n    TUKANG__15 TUKANG__16 TUKANG__17 PENATA PENATA_ PENATA1_1 MEKANIK SENIMAN_\n243          0          0          0      0       0         0       0        0\n244          0          0          0      0       0         0       0        0\n    TABIB PARAJI_ PERANCA_ PENTER_ IMAM_M PENDETA PASTOR WARTAWAN USTADZ JURU_M\n243     0       0        0       0      0       0      0        0      0      0\n244     0       0        0       0      0       0      0        0      0      0\n    PROMOT ANGGOTA_ ANGGOTA1 ANGGOTA1_1 PRESIDEN WAKIL_PRES ANGGOTA1_2\n243      0        0        0          0        0          0          0\n244      0        0        0          0        0          0          0\n    ANGGOTA1_3 DUTA_B GUBERNUR WAKIL_GUBE BUPATI WAKIL_BUPA WALIKOTA WAKIL_WALI\n243          0      0        0          0      0          0        0          0\n244          0      0        0          0      0          0        0          0\n    ANGGOTA1_4 ANGGOTA1_5 DOSEN GURU PILOT PENGACARA_ NOTARIS ARSITEK AKUNTA_\n243          0          0     0    0     0          0       0       0       0\n244          0          0     0    0     0          0       0       0       0\n    KONSUL_ DOKTER BIDAN PERAWAT APOTEK_ PSIKIATER PENYIA_ PENYIA1 PELAUT\n243       0      0     0       0       0         0       0       0      0\n244       0      0     0       0       0         0       0       0      0\n    PENELITI SOPIR PIALAN PARANORMAL PEDAGA_ PERANG_ KEPALA_ BIARAW_ WIRASWAST_\n243        0     0      0          0       0       0       0       0          0\n244        0     0      0          0       0       0       0       0          0\n    LAINNYA_12 LUAS_DESA KODE_DES_3 DESA_KEL_1 KODE_12\n243          0         0       <NA>       <NA>       0\n244          0         0       <NA>       <NA>       0\n                          geometry\n243 MULTIPOLYGON (((106.8612 -6...\n244 MULTIPOLYGON (((106.8504 -6...\n\n\nThere are 2 rows containing ‘NA’ values. However, the data is big, we need to find columns with missing NA values to remove it.\n\nnames(which(colSums(is.na(bd_jakarta))>0))\n\n[1] \"KAB_KOTA\"   \"KECAMATAN\"  \"DESA_KELUR\" \"GENERATED\"  \"KODE_DES_1\"\n[6] \"KODE_DES_3\" \"DESA_KEL_1\"\n\n\nWe can see that there are two particular rows with missing values for KAB_KOTA (City), KECAMATAN (District) and DESA_KELUR (Village).\nHence, we remove rows with NA value in DESA_KELUR. There are other columns with NA present as well, however, since we are only looking at the sub-district level, it is most appropriate to remove DESA_KELUR.\n\nbd_jakarta <- na.omit(bd_jakarta,c(\"DESA_KELUR\"))\n\nTo double check if the rows with missing values are removed\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 0 features and 161 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n  [1] OBJECT_ID  KODE_DESA  DESA       KODE       PROVINSI   KAB_KOTA  \n  [7] KECAMATAN  DESA_KELUR JUMLAH_PEN JUMLAH_KK  LUAS_WILAY KEPADATAN \n [13] PERPINDAHA JUMLAH_MEN PERUBAHAN  WAJIB_KTP  SILAM      KRISTEN   \n [19] KHATOLIK   HINDU      BUDHA      KONGHUCU   KEPERCAYAA PRIA      \n [25] WANITA     BELUM_KAWI KAWIN      CERAI_HIDU CERAI_MATI U0        \n [31] U5         U10        U15        U20        U25        U30       \n [37] U35        U40        U45        U50        U55        U60       \n [43] U65        U70        U75        TIDAK_BELU BELUM_TAMA TAMAT_SD  \n [49] SLTP       SLTA       DIPLOMA_I  DIPLOMA_II DIPLOMA_IV STRATA_II \n [55] STRATA_III BELUM_TIDA APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN \n [61] NELAYAN    AGAMA_DAN  PELAJAR_MA TENAGA_KES PENSIUNAN  LAINNYA   \n [67] GENERATED  KODE_DES_1 BELUM_     MENGUR_    PELAJAR_   PENSIUNA_1\n [73] PEGAWAI_   TENTARA    KEPOLISIAN PERDAG_    PETANI     PETERN_   \n [79] NELAYAN_1  INDUSTR_   KONSTR_    TRANSP_    KARYAW_    KARYAW1   \n [85] KARYAW1_1  KARYAW1_12 BURUH      BURUH_     BURUH1     BURUH1_1  \n [91] PEMBANT_   TUKANG     TUKANG_1   TUKANG_12  TUKANG__13 TUKANG__14\n [97] TUKANG__15 TUKANG__16 TUKANG__17 PENATA     PENATA_    PENATA1_1 \n[103] MEKANIK    SENIMAN_   TABIB      PARAJI_    PERANCA_   PENTER_   \n[109] IMAM_M     PENDETA    PASTOR     WARTAWAN   USTADZ     JURU_M    \n[115] PROMOT     ANGGOTA_   ANGGOTA1   ANGGOTA1_1 PRESIDEN   WAKIL_PRES\n[121] ANGGOTA1_2 ANGGOTA1_3 DUTA_B     GUBERNUR   WAKIL_GUBE BUPATI    \n[127] WAKIL_BUPA WALIKOTA   WAKIL_WALI ANGGOTA1_4 ANGGOTA1_5 DOSEN     \n[133] GURU       PILOT      PENGACARA_ NOTARIS    ARSITEK    AKUNTA_   \n[139] KONSUL_    DOKTER     BIDAN      PERAWAT    APOTEK_    PSIKIATER \n[145] PENYIA_    PENYIA1    PELAUT     PENELITI   SOPIR      PIALAN    \n[151] PARANORMAL PEDAGA_    PERANG_    KEPALA_    BIARAW_    WIRASWAST_\n[157] LAINNYA_12 LUAS_DESA  KODE_DES_3 DESA_KEL_1 KODE_12    geometry  \n<0 rows> (or 0-length row.names)\n\n\n\n\n1.2.2 Transforming Coordinates\nPreviously as mentioned it uses the WGS 84 coordinate system. The data is using a Geographic projected system, however, this is system is not appropriate since we need to use distance and area measures.\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nTherefore, we use st_transform() and not st_set_crs() as st_set_crs() assigns the EPSG code to the data frame. And we need to transform the data frame from geographic to projected coordinate system. We will be using crs=23845 (found from the EPSG for Indonesia).\n\nbd_jakarta <- st_transform(bd_jakarta, 23845)\n\nCheck if CRS has been assigned\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: EPSG:23845 \n  wkt:\nPROJCRS[\"DGN95 / Indonesia TM-3 zone 54.1\",\n    BASEGEOGCRS[\"DGN95\",\n        DATUM[\"Datum Geodesi Nasional 1995\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4755]],\n    CONVERSION[\"Indonesia TM-3 zone 54.1\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",139.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9999,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre.\"],\n        AREA[\"Indonesia - onshore east of 138°E.\"],\n        BBOX[-9.19,138,-1.49,141.01]],\n    ID[\"EPSG\",23845]]\n\n\n\n\n1.2.3 Removal of the Outer Island\nWe have done our basic pre-processing, lets quickly visualize the data\n\nplot(st_geometry(bd_jakarta))\n\n\n\n\nAs we can see from the diagram, bd_jakarta includes both mainland and outer islands. And since we don’t require the outer islands (as per the requirements), we can remove them.\nWe know that the date is grouped by KAB_KOTA (City), KECAMATAN (Sub-District) and DESA_KELUR (Village). Now, lets plot the map and see how we can use KAB_KOTA to remove the outer islands.\n\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\nFrom the map, we can see that all the cities in Jakarta start with ‘Jakarta’ as their prefix and hence, ‘Kepulauan Seribu’ are the other outer islands. When translated in English, the name means ‘Thousand Islands’. Now we know what to remove, and we shall proceed with that.\n\nbd_jakarta <- filter(bd_jakarta, KAB_KOTA != \"KEPULAUAN SERIBU\")\n\nNow, lets double check if the outer islands have been removed.\n\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\n\n\n1.2.4 To retain the first 9 columns as requested\n\nbd_jakarta <- bd_jakarta[, 0:9]\n\n\n\n1.2.5 Renaming columns to English\n\nbd_jakarta <- bd_jakarta %>% \n  dplyr::rename(\n    Object_ID=OBJECT_ID,\n    Village_Code=KODE_DESA, \n    Village=DESA,\n    Code=KODE,\n    Province=PROVINSI, \n    City=KAB_KOTA, \n    District=KECAMATAN, \n    Sub_District=DESA_KELUR,\n    Total_Population=JUMLAH_PEN\n    )"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#importing-eda",
    "href": "Take-home_Ex2/Take_home_Ex2.html#importing-eda",
    "title": "Take Home Exercise 2",
    "section": "2.1 Importing EDA",
    "text": "2.1 Importing EDA\nFor this take home exercise 2, we will be working on data from July 2021 to June 2022, as a result we will be having several excel files.\n\njul2021 <- read_xlsx(\"data/Aspatial/Data Vaksinasi Berbasis Kelurahan (31 Juli 2021).xlsx\")\n\nglimpse(jul2021)\n\nRows: 268\nColumns: 27\n$ `KODE KELURAHAN`                             <chr> NA, \"3172051003\", \"317304…\n$ `WILAYAH KOTA`                               <chr> NA, \"JAKARTA UTARA\", \"JAK…\n$ KECAMATAN                                    <chr> NA, \"PADEMANGAN\", \"TAMBOR…\n$ KELURAHAN                                    <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\"…\n$ SASARAN                                      <dbl> 8941211, 23947, 29381, 29…\n$ `BELUM VAKSIN`                               <dbl> 4441501, 12333, 13875, 18…\n$ `JUMLAH\\r\\nDOSIS 1`                          <dbl> 4499710, 11614, 15506, 10…\n$ `JUMLAH\\r\\nDOSIS 2`                          <dbl> 1663218, 4181, 4798, 3658…\n$ `TOTAL VAKSIN\\r\\nDIBERIKAN`                  <dbl> 6162928, 15795, 20304, 14…\n$ `LANSIA\\r\\nDOSIS 1`                          <dbl> 502579, 1230, 2012, 865, …\n$ `LANSIA\\r\\nDOSIS 2`                          <dbl> 440910, 1069, 1729, 701, …\n$ `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN`          <dbl> 943489, 2299, 3741, 1566,…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 1`                  <dbl> 1052883, 3333, 2586, 2837…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 2`                  <dbl> 666009, 2158, 1374, 1761,…\n$ `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN`   <dbl> 1718892, 5491, 3960, 4598…\n$ `GOTONG ROYONG\\r\\nDOSIS 1`                   <dbl> 56660, 78, 122, 174, 71, …\n$ `GOTONG ROYONG\\r\\nDOSIS 2`                   <dbl> 38496, 51, 84, 106, 57, 7…\n$ `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN`    <dbl> 95156, 129, 206, 280, 128…\n$ `TENAGA KESEHATAN\\r\\nDOSIS 1`                <dbl> 76397, 101, 90, 215, 73, …\n$ `TENAGA KESEHATAN\\r\\nDOSIS 2`                <dbl> 67484, 91, 82, 192, 67, 3…\n$ `TENAGA KESEHATAN TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl> 143881, 192, 172, 407, 14…\n$ `TAHAPAN 3\\r\\nDOSIS 1`                       <dbl> 2279398, 5506, 9012, 5408…\n$ `TAHAPAN 3\\r\\nDOSIS 2`                       <dbl> 446028, 789, 1519, 897, 4…\n$ `TAHAPAN 3 TOTAL\\r\\nVAKSIN DIBERIKAN`        <dbl> 2725426, 6295, 10531, 630…\n$ `REMAJA\\r\\nDOSIS 1`                          <dbl> 531793, 1366, 1684, 1261,…\n$ `REMAJA\\r\\nDOSIS 2`                          <dbl> 4291, 23, 10, 1, 1, 8, 6,…\n$ `REMAJA TOTAL\\r\\nVAKSIN DIBERIKAN`           <dbl> 536084, 1389, 1694, 1262,…\n\n\nFrom opening up the excel file till February 2022, the number of columns is 27. However, from March 2022 the number of columns is 34. Upon identifying the difference between the number of columns, the data files from March 2022 has a separate column for 3rd dosage, where has all the data files before that don’t have 3rd dosage column."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#creating-aspatial-data-pre-processing-function",
    "href": "Take-home_Ex2/Take_home_Ex2.html#creating-aspatial-data-pre-processing-function",
    "title": "Take Home Exercise 2",
    "section": "2.2 Creating Aspatial Data Pre-Processing Function",
    "text": "2.2 Creating Aspatial Data Pre-Processing Function\nFor take home exercise 2, we don’t require all the columns. Only the following columns are required -\nKODE KELURAHAN (Sub-District Code)\nKELURAHAN (Sub-District)\nSASARAN (Target)\nBELUM VASKIN (Yet to be vaccinated / Not yet vaccinated)\nThis solves the issue of some months having extra columns. However, we need to create an ‘Date’ column that shows the month and year of the observation, which is originally the file name. Each file has the naming convention ’Data Vaksinasi Berbasis Keluarahan (DD Month YYYY).\nWe will be combining the mentioned steps into a function\n\n# takes in an aspatial data filepath and returns a processed output\naspatial_preprocess <- function(filepath){\n  # We have to remove the first row of the file (subheader row) and hence, we use [-1,] to remove it.\n  result_file <- read_xlsx(filepath)[-1,]\n  \n  # We then create the Date Column, the format of our files is: Data Vaksinasi Berbasis Kelurahan (DD Month YYYY)\n  # While the start is technically \"(\", \"(\" is part of a regular expression and leads to a warning message, so we'll use \"Kelurahan\" instead. The [[1]] refers to the first element in the list.\n  # We're loading it as DD-Month-YYYY format\n  # We use the length of the filepath '6' to get the end index (which has our Date)\n  # as such, the most relevant functions are substr (returns a substring) and either str_locate (returns location of substring as an integer matrix) or gregexpr (returns a list of locations of substring)\n  # reference https://stackoverflow.com/questions/14249562/find-the-location-of-a-character-in-string\n  startpoint <- gregexpr(pattern=\"Kelurahan\", filepath)[[1]] + 11\n  \n  result_file$Date <- substr(filepath, startpoint, nchar(filepath)-6)\n  \n  # Retain the Relevant Columns\n  result_file <- result_file %>% \n    select(\"Date\", \n           \"KODE KELURAHAN\", \n           \"KELURAHAN\", \n           \"SASARAN\", \n           \"BELUM VAKSIN\")\n  return(result_file)\n}"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#feed-files-into-aspatial-function",
    "href": "Take-home_Ex2/Take_home_Ex2.html#feed-files-into-aspatial-function",
    "title": "Take Home Exercise 2",
    "section": "2.3 Feed files into Aspatial function",
    "text": "2.3 Feed files into Aspatial function\nInstead of manually feeding the files, line by line, we will be using the function list.files() and lapply() to get our process done quicker.\n\n# in the folder 'data/aspatial', find files with the extension '.xlsx' and add it to our fileslist \n# the full.names=TRUE prepends the directory path to the file names, giving a relative file path - otherwise, only the file names (not the paths) would be returned \n# reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/list.files.html\nfileslist <-list.files(path = \"data/Aspatial\", pattern = \"*.xlsx\", full.names=TRUE)\n\n# afterwards, for every element in fileslist, apply aspatial_process function\ndflist <- lapply(seq_along(fileslist), function(x) aspatial_preprocess(fileslist[x]))\n\nWe will then convert the dflist into an actual dataframe with ldply() using the below code\n\nvaccination_jakarta <- ldply(dflist, data.frame)\n\nNow, lets take a look into our data\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date           <chr> \"27 Februari 2022\", \"27 Februari 2022\", \"27 Februari 20…\n$ KODE.KELURAHAN <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"3175031003\",…\n$ KELURAHAN      <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"BAMBU…\n$ SASARAN        <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 47898, …\n$ BELUM.VAKSIN   <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 3772, 7…"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#formatting-date-column",
    "href": "Take-home_Ex2/Take_home_Ex2.html#formatting-date-column",
    "title": "Take Home Exercise 2",
    "section": "2.4 Formatting Date Column",
    "text": "2.4 Formatting Date Column\nThe Dates are in Bahasa Indonesia, and hence, we need to translate them to English for ease of use. However, since the values in Date column were derived from sub-strings, they are in a string format and thus, first need to be converted to datetime.\n\n# parses the 'Date' column into Month(Full Name)-YYYY datetime objects\n# reference: https://stackoverflow.com/questions/53380650/b-y-date-conversion-gives-na\n\n# locale=\"ind\" means that the locale has been set as Indonesia\nSys.setlocale(locale=\"ind\")\n\n[1] \"LC_COLLATE=Indonesian_Indonesia.1252;LC_CTYPE=Indonesian_Indonesia.1252;LC_MONETARY=Indonesian_Indonesia.1252;LC_NUMERIC=C;LC_TIME=Indonesian_Indonesia.1252\"\n\n\n\nvaccination_jakarta$Date <- c(vaccination_jakarta$Date) %>% \n  as.Date(vaccination_jakarta$Date, format =\"%d %B %Y\")\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date           <date> 2022-02-27, 2022-02-27, 2022-02-27, 2022-02-27, 2022-0~\n$ KODE.KELURAHAN <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"3175031003\",~\n$ KELURAHAN      <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"BAMBU~\n$ SASARAN        <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 47898, ~\n$ BELUM.VAKSIN   <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 3772, 7~"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#rename-columns-into-english",
    "href": "Take-home_Ex2/Take_home_Ex2.html#rename-columns-into-english",
    "title": "Take Home Exercise 2",
    "section": "2.5 Rename columns into English",
    "text": "2.5 Rename columns into English\n\n# renames the columns in the style New_Name = OLD_NAME\nvaccination_jakarta <- vaccination_jakarta %>% \n  dplyr::rename(\n    Date=Date,\n    Sub_District_Code=KODE.KELURAHAN,\n    Sub_District=KELURAHAN, \n    Target=SASARAN, \n    Not_Yet_Vaccinated=BELUM.VAKSIN\n    )\n\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date               <date> 2022-02-27, 2022-02-27, 2022-02-27, 2022-02-27, 20~\n$ Sub_District_Code  <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"31750310~\n$ Sub_District       <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"B~\n$ Target             <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 478~\n$ Not_Yet_Vaccinated <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 377~"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#further-data-processing",
    "href": "Take-home_Ex2/Take_home_Ex2.html#further-data-processing",
    "title": "Take Home Exercise 2",
    "section": "2.6 Further data processing",
    "text": "2.6 Further data processing\nFurther perform any pre-processing to check out for anything we might have missed.\n\nvaccination_jakarta[rowSums(is.na(vaccination_jakarta))!=0,]\n\n[1] Date               Sub_District_Code  Sub_District       Target            \n[5] Not_Yet_Vaccinated\n<0 rows> (or 0-length row.names)\n\n\nFrom the output, we can see there are no missing values."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#initial-exploratory-data-analysis",
    "href": "Take-home_Ex2/Take_home_Ex2.html#initial-exploratory-data-analysis",
    "title": "Take Home Exercise 2",
    "section": "3.1 Initial Exploratory Data Analysis",
    "text": "3.1 Initial Exploratory Data Analysis\nWe have both our Geospatial and Aspatial data, we need to join them. However, we need to first find a common header to join them.\n\ncolnames(bd_jakarta)\n\n [1] \"Object_ID\"        \"Village_Code\"     \"Village\"          \"Code\"            \n [5] \"Province\"         \"City\"             \"District\"         \"Sub_District\"    \n [9] \"Total_Population\" \"geometry\"        \n\n\n\ncolnames(vaccination_jakarta)\n\n[1] \"Date\"               \"Sub_District_Code\"  \"Sub_District\"      \n[4] \"Target\"             \"Not_Yet_Vaccinated\"\n\n\nWe can see that both have Sub_District and hence we can join them by the Sub_District and Sub_District_Code.\n\n# joins vaccination_jakarta to jakarta based on Sub_District and  Sub_District_Code\ncombined_jakarta <- left_join(bd_jakarta, vaccination_jakarta,\n                              by=c(\n                                \"Village_Code\"=\"Sub_District_Code\", \n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nSubcategorize the data into ‘Target population to be Vaccinated’ , ‘Not Yet Vaccinated Population’ and ‘Total Population’\n\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target Count\")\n\nnot_yet_vaccinated = tm_shape(combined_jakarta)+\n  tm_fill(\"Not_Yet_Vaccinated\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Not Yet Vaccinated Count\")\n\ntotal_population = tm_shape(combined_jakarta)+\n  tm_fill(\"Total_Population\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Total Population\")\n\ntmap_arrange(target, not_yet_vaccinated, total_population)\n\n\n\n\nThere seems to be still be a ‘Missing’ value in the Target and Not_Yet_Vaccinated maps. Even though, when we had previously checked for missing values, it didn’t show any missing values. However, we shall double check again.\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 0 features and 9 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n [1] Object_ID        Village_Code     Village          Code            \n [5] Province         City             District         Sub_District    \n [9] Total_Population geometry        \n<0 rows> (or 0-length row.names)\n\n\n\nvaccination_jakarta[rowSums(is.na(vaccination_jakarta))!=0,]\n\n[1] Date               Sub_District_Code  Sub_District       Target            \n[5] Not_Yet_Vaccinated\n<0 rows> (or 0-length row.names)\n\n\nThere are no missing values in our dataframes. Therefore, the most likely reasons for the missing values must be due to mismatched values when we perform the left-join of the Geospatial and Aspatial data."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#finding-mismatched-sub-district-records",
    "href": "Take-home_Ex2/Take_home_Ex2.html#finding-mismatched-sub-district-records",
    "title": "Take Home Exercise 2",
    "section": "3.2 Finding mismatched sub-district records",
    "text": "3.2 Finding mismatched sub-district records\nSince, we had conducted left-join using the Sub-District, there must be a mismatch in the naming of the subdistricts. Lets check it by looking at the unique subdistrict names in both bd_jakarta and vaccination_jakarta\n\njakarta_subdistrict <- c(bd_jakarta$Sub_District)\nvaccination_subdistrict <- c(vaccination_jakarta$Sub_District)\n\nunique(jakarta_subdistrict[!(jakarta_subdistrict %in% vaccination_subdistrict)])\n\n[1] \"KRENDANG\"             \"RAWAJATI\"             \"TENGAH\"              \n[4] \"BALEKAMBANG\"          \"PINANGRANTI\"          \"JATIPULO\"            \n[7] \"PALMERIAM\"            \"KRAMATJATI\"           \"HALIM PERDANA KUSUMA\"\n\n\n\nunique(vaccination_subdistrict[!(vaccination_subdistrict %in% jakarta_subdistrict)])\n\n [1] \"BALE KAMBANG\"          \"HALIM PERDANA KUSUMAH\" \"JATI PULO\"            \n [4] \"KAMPUNG TENGAH\"        \"KERENDANG\"             \"KRAMAT JATI\"          \n [7] \"PAL MERIAM\"            \"PINANG RANTI\"          \"PULAU HARAPAN\"        \n[10] \"PULAU KELAPA\"          \"PULAU PANGGANG\"        \"PULAU PARI\"           \n[13] \"PULAU TIDUNG\"          \"PULAU UNTUNG JAWA\"     \"RAWA JATI\"            \n\n\nFrom above there are same names in both but are just written in different ways. However, there are 6 words in the vaccination_subdistrict which are not in the jakarta_subdistrict. We need to take a look into that after we first correct the mismatched values.\n\n# initialise a dataframe of our cases vs bd subdistrict spelling\nspelling <- data.frame(\n  Aspatial_Cases=c(\"BALE KAMBANG\", \"HALIM PERDANA KUSUMAH\", \"JATI PULO\", \"KAMPUNG TENGAH\", \"KERENDANG\", \"KRAMAT JATI\", \"PAL MERIAM\", \"PINANG RANTI\", \"RAWA JATI\"),\n  Geospatial_BD=c(\"BALEKAMBAG\", \"HALIM PERDANA KUSUMA\", \"JATIPULO\", \"TENGAH\", \"KRENDANG\", \"KRAMATJATI\", \"PALMERIAM\", \"PINANGRANTI\", \"RAWAJATI\")\n  )\n\n# with dataframe a input, outputs a kable\nlibrary(knitr)\nlibrary(kableExtra)\nkable(spelling, caption=\"Mismatched Records\") %>%\n  kable_material(\"hover\", latex_options=\"scale_down\")\n\n\n\nMismatched Records\n \n  \n    Aspatial_Cases \n    Geospatial_BD \n  \n \n\n  \n    BALE KAMBANG \n    BALEKAMBAG \n  \n  \n    HALIM PERDANA KUSUMAH \n    HALIM PERDANA KUSUMA \n  \n  \n    JATI PULO \n    JATIPULO \n  \n  \n    KAMPUNG TENGAH \n    TENGAH \n  \n  \n    KERENDANG \n    KRENDANG \n  \n  \n    KRAMAT JATI \n    KRAMATJATI \n  \n  \n    PAL MERIAM \n    PALMERIAM \n  \n  \n    PINANG RANTI \n    PINANGRANTI \n  \n  \n    RAWA JATI \n    RAWAJATI \n  \n\n\n\n\n\nAs we can see these records have the same name, except that there is no standardization. Therefore, there is a mismatch between them. Let’s correct this mismatch\n\n# We are replacing the mistmatched values in jakarta with the correct value\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'BALEKAMBANG'] <- 'BALE KAMBANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'HALIM PERDANA KUSUMA'] <- 'HALIM PERDANA KUSUMAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'JATIPULO'] <- 'JATI PULO'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KALI BARU'] <- 'KALIBARU'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'TENGAH'] <- 'KAMPUNG TENGAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRAMATJATI'] <- 'KRAMAT JATI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRENDANG'] <- 'KERENDANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PALMERIAM'] <- 'PAL MERIAM'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PINANGRANTI'] <- 'PINANG RANTI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'RAWAJATI'] <- 'RAWA JATI'\n\nThere are 6 subdistrict names that we say in vaccination_jakarta which were not present in jakarta. This ideally suggests that these districts are not a part of Jakarta, Therefore we need to remove them.\n\nvaccination_jakarta <- vaccination_jakarta[!(vaccination_jakarta$Sub_District==\"PULAU HARAPAN\" | vaccination_jakarta$Sub_District==\"PULAU KELAPA\" | vaccination_jakarta$Sub_District==\"PULAU PANGGANG\" | vaccination_jakarta$Sub_District==\"PULAU PARI\" | vaccination_jakarta$Sub_District==\"PULAU TIDUNG\" | vaccination_jakarta$Sub_District==\"PULAU UNTUNG JAWA\"), ]"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#rejoin-exploratory-data-analysis",
    "href": "Take-home_Ex2/Take_home_Ex2.html#rejoin-exploratory-data-analysis",
    "title": "Take Home Exercise 2",
    "section": "3.3 Rejoin Exploratory Data Analysis",
    "text": "3.3 Rejoin Exploratory Data Analysis\n\n# joins vaccination_jakarta to bd_jakarta based on Sub_District and  Sub_District_Code\ncombined_jakarta <- left_join(bd_jakarta, vaccination_jakarta,\n                              by=c(\n                                \"Village_Code\"=\"Sub_District_Code\", \n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nCheck if there are any further NA values\n\ncombined_jakarta[rowSums(is.na(combined_jakarta))!=0,]\n\nSimple feature collection with 0 features and 12 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n [1] Object_ID          Village_Code       Village            Code              \n [5] Province           City               District           Sub_District      \n [9] Total_Population   Date               Target             Not_Yet_Vaccinated\n[13] geometry          \n<0 rows> (or 0-length row.names)\n\n\nRelook the data into ‘Target population to be Vaccinated’ , ‘Not Yet Vaccinated Population’ and ‘Total Population’\n\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target Count\")\n\nnot_yet_vaccinated = tm_shape(combined_jakarta)+\n  tm_fill(\"Not_Yet_Vaccinated\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Not Yet Vaccinated Count\")\n\ntotal_population = tm_shape(combined_jakarta)+\n  tm_fill(\"Total_Population\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Total Population\")\n\ntmap_arrange(target, not_yet_vaccinated, total_population)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#convert-dataframe-to-sf",
    "href": "Take-home_Ex2/Take_home_Ex2.html#convert-dataframe-to-sf",
    "title": "Take Home Exercise 2",
    "section": "4.1 Convert dataframe to SF",
    "text": "4.1 Convert dataframe to SF\n\ncombined_jakarta <- st_as_sf(combined_jakarta)\n\n# need to join our previous dataframes with the geospatial data to ensure that geometry column is present\nvaccination_rate <- vaccination_rate%>% left_join(bd_jakarta, by=c(\"Sub_District\"=\"Sub_District\"))\nvaccination_rate <- st_as_sf(vaccination_rate)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#jenks-choropleth-mapping",
    "href": "Take-home_Ex2/Take_home_Ex2.html#jenks-choropleth-mapping",
    "title": "Take Home Exercise 2",
    "section": "5.1 Jenks Choropleth Mapping",
    "text": "5.1 Jenks Choropleth Mapping\n\n# using the jenks method, with 6 classes for human eye\ntmap_mode(\"plot\")\ntm_shape(vaccination_rate)+\n  tm_fill(\"2021-07-31\", \n          n= 6,\n          style = \"jenks\", \n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Vaccination Rate in July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlot for all 12 months. Adopt a helper function to help us do it.\n\n# input: the dataframe and the variable name - in this case, the month \n# with style=\"jenks\" for the jenks classification method\njenks_plot <- function(df, varname) {\n  tm_shape(vaccination_rate) +\n    tm_polygons() +\n  tm_shape(df) +\n    tm_fill(varname, \n          n= 6,\n          style = \"jenks\", \n          title = \"Vaccination Rate\") +\n    tm_layout(main.title = varname,\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          legend.height = 0.45, \n          legend.width = 0.35,\n          frame = TRUE) +\n    tm_borders(alpha = 0.5)\n}\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2021-07-31\"),\n             jenks_plot(vaccination_rate, \"2021-08-31\"),\n             jenks_plot(vaccination_rate, \"2021-09-30\"),\n             jenks_plot(vaccination_rate, \"2021-10-31\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2021-11-30\"),\n             jenks_plot(vaccination_rate, \"2021-12-31\"),\n             jenks_plot(vaccination_rate, \"2022-01-31\"),\n             jenks_plot(vaccination_rate, \"2022-02-27\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2022-03-31\"),\n             jenks_plot(vaccination_rate, \"2022-04-30\"),\n             jenks_plot(vaccination_rate, \"2022-05-31\"),\n             jenks_plot(vaccination_rate, \"2022-06-30\"))\n\n\n\n\nObservations from the plotted map\nEach plotted map has been arranged monthly and has it own relative vaccination rate. As observed, the colours turns darker over time. As the ranges and gradually grow larger over time, more people are getting vaccinated.\nBy looking at the early stages between July 2021 and October 2021, there is a darkly-coloured cluster around the north of Jakarta which include KAMAL MUARA and HALIM PERDANA KUSUMAH sub-district with the highest vaccination rate.\nAs for other sub districts between (November 2021 ~ February 2022, other sub districts have darken in colour and the HALIM PERDANA KUSUMAH still remains the sub-district with the highest vaccination rate.\nIn the later stages of vaccination from March 2022, more sub-districts have lower vaccination rate (lighter colour) especially for the most of the sub-districts in the North and West. However, HALIM PERDANA KUSUMAH still remains the sub-district with the highest vaccination rate.\nChecking for sub-districts with highest vaccination rate according to month\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-07-31`)]\n\n[1] \"KAMAL MUARA\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-08-31`)]\n\n[1] \"KAMAL MUARA\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-09-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-10-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-11-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-12-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-01-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-02-27`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-03-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-04-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-05-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-06-31`)]\n\ncharacter(0)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#calculation-of-local-gi-of-monthly-vaccination-rate",
    "href": "Take-home_Ex2/Take_home_Ex2.html#calculation-of-local-gi-of-monthly-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "6.1 Calculation of Local GI* of monthly vaccination rate",
    "text": "6.1 Calculation of Local GI* of monthly vaccination rate\n\n# Make new vaccination attribute table with Date, Sub_District, Target, Not_Yet_Vaccinated\nvaccination_table <- combined_jakarta %>% select(10, 8, 11, 12) %>% st_drop_geometry()\n\n# Adding a new field for Vaccination_Rate\nvaccination_table$Vaccination_Rate <- ((vaccination_table$Target - vaccination_table$Not_Yet_Vaccinated) / vaccination_table$Target) *100\n\n# Vaccination attribute table with just Date, Sub_District, Vaccination_Rate\nvaccination_table <- tibble(vaccination_table %>% select(1,2,5))"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#create-time-series-cube",
    "href": "Take-home_Ex2/Take_home_Ex2.html#create-time-series-cube",
    "title": "Take Home Exercise 2",
    "section": "6.2 Create Time Series Cube",
    "text": "6.2 Create Time Series Cube\n\nvaccination_rate_st <- spacetime(vaccination_table, bd_jakarta,\n                          .loc_col = \"Sub_District\",\n                          .time_col = \"Date\")\n\nVerify if vaccination_rate_st is indeed a space-time cube by using the is_spacetime_cube() of sfdep package.\n\nis_spacetime_cube(vaccination_rate_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#deriving-spatial-weights",
    "href": "Take-home_Ex2/Take_home_Ex2.html#deriving-spatial-weights",
    "title": "Take Home Exercise 2",
    "section": "6.3 Deriving Spatial Weights",
    "text": "6.3 Deriving Spatial Weights\nCalculation of local Gi* weights will be done. However, before that we need derive the spatial weights. The below code chunk is used to identify neighbors and derive an inverse distance weights.\n\nvaccination_rate_nb <- vaccination_rate_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale=1,\n                                  alpha=1),\n         .before=1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\nNote that\n\nactivate() is used to activate the geometry context\nmutate() is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\nThe dataset provided has neighbours and weights for each time slicing\n\nhead(vaccination_rate_nb)\n\n# A tibble: 6 x 5\n  Date       Sub_District  Vaccination_Rate nb        wt       \n  <date>     <chr>                    <dbl> <list>    <list>   \n1 2021-07-31 KEAGUNGAN                 53.3 <int [6]> <dbl [6]>\n2 2021-07-31 GLODOK                    61.6 <int [7]> <dbl [7]>\n3 2021-07-31 HARAPAN MULIA             49.7 <int [6]> <dbl [6]>\n4 2021-07-31 CEMPAKA BARU              46.7 <int [7]> <dbl [7]>\n5 2021-07-31 PASAR BARU                59.3 <int [9]> <dbl [9]>\n6 2021-07-31 KARANG ANYAR              52.2 <int [7]> <dbl [7]>\n\n\nset.seed() will be use before performing simulation to ensure that the computation is reproducible. When a random number generator is used, the results can be different each time the code is run, which makes it difficult to reproduce results. By setting the seed to a specific value (e.g., set.seed(1234)), the same random numbers will be generated each time the code is run, making the results reproducible and consistent.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#calculation-of-gi-value",
    "href": "Take-home_Ex2/Take_home_Ex2.html#calculation-of-gi-value",
    "title": "Take Home Exercise 2",
    "section": "6.4 Calculation of GI* value",
    "text": "6.4 Calculation of GI* value\nThe calculation of the Gi* value for each sub-district where we group by date\n\ngi_values <- vaccination_rate_nb |>\n  group_by(Date) |>\n  mutate(gi_values = local_gstar_perm(\n    Vaccination_Rate, nb, wt, nsim=99)) |>\n      tidyr::unnest(gi_values)\n\ngi_values\n\n# A tibble: 3,132 x 13\n# Groups:   Date [12]\n   Date       Sub_Di~1 Vacci~2 nb    wt    gi_star    e_gi  var_gi p_value p_sim\n   <date>     <chr>      <dbl> <lis> <lis>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 2021-07-31 KEAGUNG~    53.3 <int> <dbl>   2.44  0.00383 2.13e-8 1.46e-2  0.02\n 2 2021-07-31 GLODOK      61.6 <int> <dbl>   3.85  0.00384 1.56e-8 1.18e-4  0.02\n 3 2021-07-31 HARAPAN~    49.7 <int> <dbl>   0.309 0.00382 2.20e-8 7.57e-1  0.84\n 4 2021-07-31 CEMPAKA~    46.7 <int> <dbl>  -1.05  0.00383 1.53e-8 2.96e-1  0.34\n 5 2021-07-31 PASAR B~    59.3 <int> <dbl>   2.71  0.00383 1.38e-8 6.82e-3  0.02\n 6 2021-07-31 KARANG ~    52.2 <int> <dbl>   1.67  0.00382 2.17e-8 9.49e-2  0.1 \n 7 2021-07-31 MANGGA ~    51.6 <int> <dbl>   1.35  0.00384 1.80e-8 1.77e-1  0.22\n 8 2021-07-31 PETOJO ~    47.2 <int> <dbl>  -0.179 0.00382 1.92e-8 8.58e-1  0.96\n 9 2021-07-31 SENEN       54.4 <int> <dbl>   1.51  0.00382 1.20e-8 1.32e-1  0.1 \n10 2021-07-31 BUNGUR      52.8 <int> <dbl>   0.797 0.00385 1.54e-8 4.25e-1  0.48\n# ... with 3,122 more rows, 3 more variables: p_folded_sim <dbl>,\n#   skewness <dbl>, kurtosis <dbl>, and abbreviated variable names\n#   1: Sub_District, 2: Vaccination_Rate"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#visualise-the-monthly-values-of-gi",
    "href": "Take-home_Ex2/Take_home_Ex2.html#visualise-the-monthly-values-of-gi",
    "title": "Take Home Exercise 2",
    "section": "6.5 Visualise the monthly values of GI*",
    "text": "6.5 Visualise the monthly values of GI*\nTo be able to visualise the Gi* values of the monthly vaccination rate, we need to join it with combined_jakarta, to be able to plot the Gi* values on the map. As the gi_values do not have any coordinates\n\njakarta_gi_values <- combined_jakarta %>%\n  left_join(gi_values)\n\njakarta_gi_values\n\nSimple feature collection with 3132 features and 23 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3644275 ymin: 663887.8 xmax: -3606237 ymax: 701380.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\nFirst 10 features:\n   Object_ID Village_Code   Village   Code    Province          City   District\n1      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n2      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n3      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n4      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n5      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n6      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n7      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n8      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n9      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n10     25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n   Sub_District Total_Population       Date Target Not_Yet_Vaccinated\n1     KEAGUNGAN            21609 2022-02-27  17387               2755\n2     KEAGUNGAN            21609 2022-04-30  17387               2593\n3     KEAGUNGAN            21609 2022-06-30  17387               2553\n4     KEAGUNGAN            21609 2021-11-30  17387               3099\n5     KEAGUNGAN            21609 2021-09-30  17387               4203\n6     KEAGUNGAN            21609 2021-08-31  17387               6054\n7     KEAGUNGAN            21609 2021-12-31  17387               2924\n8     KEAGUNGAN            21609 2022-01-31  17387               2783\n9     KEAGUNGAN            21609 2021-07-31  17387               8126\n10    KEAGUNGAN            21609 2022-03-31  17387               2675\n   Vaccination_Rate                      nb\n1          84.15483 1, 2, 39, 152, 158, 166\n2          85.08656 1, 2, 39, 152, 158, 166\n3          85.31662 1, 2, 39, 152, 158, 166\n4          82.17634 1, 2, 39, 152, 158, 166\n5          75.82677 1, 2, 39, 152, 158, 166\n6          65.18088 1, 2, 39, 152, 158, 166\n7          83.18284 1, 2, 39, 152, 158, 166\n8          83.99379 1, 2, 39, 152, 158, 166\n9          53.26393 1, 2, 39, 152, 158, 166\n10         84.61494 1, 2, 39, 152, 158, 166\n                                                                             wt\n1  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n2  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n3  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n4  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n5  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n6  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n7  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n8  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n9  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n10 0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n    gi_star        e_gi       var_gi     p_value p_sim p_folded_sim    skewness\n1  2.478859 0.003831172 8.419778e-10 0.013180341  0.02         0.01 -0.03194341\n2  2.825027 0.003827108 7.494269e-10 0.004727670  0.04         0.02 -0.06356613\n3  2.856078 0.003828377 6.980492e-10 0.004289104  0.02         0.01  0.09797571\n4  2.400280 0.003831389 1.665572e-09 0.016382514  0.02         0.01 -0.75383323\n5  2.340497 0.003839767 1.969413e-09 0.019258078  0.02         0.01 -0.45706127\n6  2.697283 0.003843766 4.229906e-09 0.006990777  0.02         0.01  0.19649652\n7  2.737341 0.003831004 9.440138e-10 0.006193804  0.02         0.01 -0.13372915\n8  2.601179 0.003833862 7.779184e-10 0.009290410  0.02         0.01 -0.51408533\n9  2.442508 0.003830045 2.133669e-08 0.014585591  0.02         0.01 -0.40723702\n10 2.631489 0.003829259 7.973859e-10 0.008501151  0.04         0.02  0.23980167\n      kurtosis                       geometry\n1  -0.81316154 MULTIPOLYGON (((-3626874 69...\n2   0.55021722 MULTIPOLYGON (((-3626874 69...\n3  -0.50749002 MULTIPOLYGON (((-3626874 69...\n4   0.54499553 MULTIPOLYGON (((-3626874 69...\n5   0.28836755 MULTIPOLYGON (((-3626874 69...\n6   0.03187466 MULTIPOLYGON (((-3626874 69...\n7  -0.02562365 MULTIPOLYGON (((-3626874 69...\n8   0.07882560 MULTIPOLYGON (((-3626874 69...\n9   0.28865884 MULTIPOLYGON (((-3626874 69...\n10 -0.20275849 MULTIPOLYGON (((-3626874 69...\n\n\nWe will proceed with visualizing the first month (July 2021). We will be plotting both the Gi* value and the p-value of Gi* for the Vaccination Rates.\nAs per take home exercise 2 requirement, we will only be plotting the significant p-value < 0.05\n\ngi_value_plot <- function(date, title) {\n  gi_star_map = tm_shape(filter(jakarta_gi_values, Date == date)) +\n    tm_fill(\"gi_star\") +\n    tm_borders(alpha=0.5) +\n    tm_view(set.zoom.limits = c(6,8)) +\n    tm_layout(main.title = paste(\"Gi* values for vaccination rates in\", title), main.title.size=0.8)\n\n  p_value_map = tm_shape(filter(jakarta_gi_values, Date == date)) +\n    tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) +\n    tm_borders(alpha=0.5) + \n    tm_layout(main.title = paste(\"p-values of Gi* for vaccination rates in\", title), main.title.size=0.8)\n\n  tmap_arrange(gi_star_map, p_value_map)\n}\n\nPlotting Gi* for all 12 months\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-07-31\", \"July 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-08-31\", \"August 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-09-30\", \"September 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-10-31\", \"October 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-11-30\", \"November 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-12-31\", \"December 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-01-31\", \"January 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-02-27\", \"February 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-03-31\", \"March 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-04-30\", \"April 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-05-31\", \"May 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-06-30\", \"June 2022\")\n\n\n\n\nStatistical conclusion\nThe p-value represents the probability of observing a clustering. A significant p-value < 0.05 suggests that an observed pattern is unlikely to have occurred by chance and may indicate the presence of a spatial process. When Gi* value > 0, it indicates sub-districts with a higher vaccination rate than average. We can view the number of sub districts with p-value < 0.05 with the code below\n\nno_of_subdistricts_freq = filter(jakarta_gi_values, p_sim < 0.05)\nas.data.frame(table(no_of_subdistricts_freq$Sub_District))\n\n                      Var1 Freq\n1                    ANCOL    1\n2             BALE KAMBANG   11\n3              BALI MESTER    4\n4                     BARU    9\n5               BATU AMPAR   12\n6          BENDUNGAN HILIR   10\n7              BIDARA CINA    9\n8      CEMPAKA PUTIH BARAT    1\n9                  CIBUBUR   10\n10                CIGANJUR   10\n11               CIJANTUNG   10\n12                  CIKINI    1\n13                  CIKOKO    8\n14          CILANDAK BARAT    3\n15          CILANDAK TIMUR    7\n16               CILILITAN    4\n17               CILINCING   10\n18                CIPAYUNG    6\n19                 CIPEDAK   10\n20  CIPINANG BESAR SELATAN    3\n21    CIPINANG BESAR UTARA    8\n22       CIPINANG CEMPEDAK   12\n23          CIPINANG MUARA    2\n24                 CIPULIR    2\n25                 CIRACAS    1\n26             DUREN SAWIT    1\n27            DURI SELATAN    1\n28                   GALUR    2\n29                  GAMBIR    5\n30                  GEDONG    3\n31                  GELORA    7\n32                  GLODOK   12\n33              GONDANGDIA    7\n34          GROGOL SELATAN    1\n35            GROGOL UTARA    1\n36   GUNUNG SAHARI SELATAN    1\n37     GUNUNG SAHARI UTARA    1\n38   HALIM PERDANA KUSUMAH   11\n39               JAGAKARSA    9\n40              JATINEGARA    1\n41         JATINEGARA KAUM    1\n42           JEMBATAN LIMA    1\n43              JOHAR BARU    1\n44                KALIBARU   10\n45                KALISARI    9\n46                   KAMAL    1\n47             KAMAL MUARA    1\n48            KAMPUNG BALI   12\n49          KAMPUNG MELAYU    3\n50            KAMPUNG RAWA    3\n51          KAMPUNG TENGAH   12\n52             KAPUK MUARA    2\n53           KARET TENGSIN    4\n54              KAYU MANIS    1\n55               KEAGUNGAN   12\n56               KEBAGUSAN    5\n57            KEBON KACANG   12\n58            KEBON MELATI   12\n59             KEBON SIRIH    7\n60              KELAPA DUA    3\n61        KELAPA DUA WETAN   11\n62     KELAPA GADING BARAT    2\n63     KELAPA GADING TIMUR    7\n64                 KLENDER    2\n65                    KOJA    2\n66                  KRAMAT    2\n67             KRAMAT JATI    3\n68                   LAGOA    9\n69           LENTENG AGUNG    9\n70            LUBANG BUAYA    5\n71             MALAKA JAYA    4\n72             MALAKA SARI    1\n73            MANGGA BESAR   12\n74       MANGGARAI SELATAN    8\n75                  MAPHAR    3\n76                 MARUNDA    1\n77                 MELAWAI    2\n78                 MENTENG    7\n79                  MUNJUL    9\n80              PAL MERIAM    1\n81                PAPANGGO    1\n82              PASAR BARU    2\n83           PASAR MANGGIS    2\n84          PEGANGSAAN DUA    3\n85           PEJATEN TIMUR    3\n86                 PEKAYON    8\n87            PENGGILINGAN    1\n88             PENJARINGAN    2\n89              PETAMBURAN   12\n90               PETOGOGAN    2\n91          PETOJO SELATAN    7\n92            PETOJO UTARA    2\n93        PETUKANGAN UTARA    1\n94            PINANG RANTI    1\n95               PINANGSIA    3\n96           PISANGAN BARU    2\n97          PISANGAN TIMUR    1\n98            PONDOK BAMBU    4\n99             PONDOK KOPI    2\n100            PONDOK LABU    9\n101            PULO GADUNG    2\n102                RAGUNAN    7\n103               RAMBUTAN    1\n104     RAWA BADAK SELATAN    1\n105       RAWA BADAK UTARA    4\n106             RAWA BARAT    1\n107             RAWA BUNGA    9\n108             ROA MALAKA    1\n109                 SELONG    2\n110           SEMPER BARAT    5\n111           SEMPER TIMUR    4\n112                SENAYAN    2\n113                  SENEN    2\n114              SRENGSENG    1\n115        SRENGSENG SAWAH    9\n116       SUKABUMI SELATAN    4\n117         SUKABUMI UTARA    3\n118           SUNTER AGUNG    1\n119            SUNTER JAYA    4\n120           TANAH SEREAL    5\n121           TANAH TINGGI    2\n122                 TANGKI   10\n123          TANJUNG BARAT    6\n124            TEBET BARAT   10\n125            TEBET TIMUR   10\n126                 TOMANG    1\n127             TUGU UTARA    5\n128                ULUJAMI    1\n\n\nFrom the table above, there are 128 sub-districts who have a significant vaccination rate p-value < 0.05 at least once during the period of 12 months. Those sub-districts that have double digits frequency have a significant p value throughout the entire 12 months."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#mann-kendall-test",
    "href": "Take-home_Ex2/Take_home_Ex2.html#mann-kendall-test",
    "title": "Take Home Exercise 2",
    "section": "7.1 Mann-Kendall Test",
    "text": "7.1 Mann-Kendall Test\n\n7.1.1 KEAGUNGAN\n\nKeagungan <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"KEAGUNGAN\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = Keagungan, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nKeagungan %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau     sl     S     D  varS\n  <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 0.455 0.0467    30  66.0  213.\n\n\nThe p-value is 0.046 which is < 0.05 hence p-value is significant. Therefore, this is a fluctuating upward significant trend.\n\n\n7.1.2 HARAPAN MULIA\n\nHARAPAN_MULIA <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"HARAPAN MULIA\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = HARAPAN_MULIA, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nHARAPAN_MULIA %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n     tau    sl     S     D  varS\n   <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.0909 0.732     6  66.0  213.\n\n\nThe p-value is 0.73 which is > 0.05 hence p-value is not significant. Therefore, this is an downward insignificant trend.\n\n\n7.1.3 ULUJAMI\n\nulujami <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"ULUJAMI\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = ulujami, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nulujami %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau      sl     S     D  varS\n  <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 0.667 0.00319    44  66.0  213.\n\n\nThe p-value is 0.003 which is < 0.05 hence p-value is significant. Therefore, this is an upward but significant trend."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_Ex2.html#ehsa-map-of-the-gi-value",
    "href": "Take-home_Ex2/Take_home_Ex2.html#ehsa-map-of-the-gi-value",
    "title": "Take Home Exercise 2",
    "section": "7.2 EHSA map of the Gi* value",
    "text": "7.2 EHSA map of the Gi* value\nFor us to find the significant hot and cold spots, there is a need to conduct the Mann Kendall test on all the subdistricts out there. Therefore, the group_by() function will be used for all subdistricts.\n\nehsa <- gi_values %>%\n  group_by(Sub_District) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\nShow significant top 10 emerging hot/cold spots area\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nemerging\n\n# A tibble: 10 x 6\n   Sub_District             tau        sl     S     D  varS\n   <chr>                  <dbl>     <dbl> <dbl> <dbl> <dbl>\n 1 PETOJO UTARA          -0.970 0.0000156   -64  66.0  213.\n 2 KAYU MANIS             0.970 0.0000156    64  66.0  213.\n 3 JATINEGARA KAUM        0.939 0.0000287    62  66.0  213.\n 4 PISANGAN BARU          0.939 0.0000287    62  66.0  213.\n 5 PASAR BARU            -0.939 0.0000288   -62  66.0  213.\n 6 DUKUH                  0.909 0.0000521    60  66.0  213.\n 7 KAMAL MUARA           -0.909 0.0000521   -60  66.0  213.\n 8 KAPUK MUARA           -0.909 0.0000521   -60  66.0  213.\n 9 KEBON KELAPA          -0.909 0.0000521   -60  66.0  213.\n10 GUNUNG SAHARI SELATAN -0.879 0.0000928   -58  66.0  213.\n\n\nemerging_hotspot_analysis() of sfdep package will be used to perform EHSA analysis. It takes a spacetime object x (i.e. vaccination_rate_st), and the quoted name of the variable of interest (i.e. Vaccinaton Rate) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default.\n\nehsa <- emerging_hotspot_analysis(\n  x = vaccination_rate_st,\n  .var = \"Vaccination_Rate\",\n  k = 1,\n  nsim = 99\n)\n\nVisualisation of distribution\n\nggplot(data = ehsa,\n       aes(x=classification, fill=classification)) + \n  geom_bar()\n\n\n\n\nThe barchart above shows that sporadic hot spots class has the highest numbers.\nLeft join of combine jakarta and ehsa together\n\njakarta_ehsa <- bd_jakarta %>%\n  left_join(ehsa, by = c(\"Sub_District\" = \"location\"))\n\nVisualisation of classification using tmap\n\n# We use the filter to filter out values with p-value < 0.05\njakarta_ehsa_sig <- jakarta_ehsa  %>%\n  filter(p_value < 0.05)\ntmap_mode(\"plot\")\ntm_shape(jakarta_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(jakarta_ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nObservation:\nOscilating coldspot is spread out evenly in Jakarta which can be found to be more around the border and in the central of Jakarta. At the same time, the oscillating hotspot is lesser than spordiac coldspot. Similar to oscilating coldspot, there is also a large number of spordiac hotspot spread out evenly around Jakarta. From the map, the patterns are not obvious and the sub districts shaded in grey are of p value > 0.05 which means that the sub districts are insignificant.\nEnd of take home exercise 2"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html",
    "href": "Take-home_Ex2/Take_home_exe2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Since late December 2019, an outbreak of a novel coronavirus disease (COVID-19; previously known as 2019-nCoV) was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. In general, COVID-19 is an acute resolved disease but it can also be deadly, with a 2% case fatality rate.\nThe COVID-19 vaccination in Indonesia is an ongoing mass immunisation in response to the COVID-19 pandemic in Indonesia. On 13 January 2021, the program commenced when President Joko Widodo was vaccinated at the presidential palace. In terms of total doses given, Indonesia ranks third in Asia and fifth in the world.\nAccording to wikipedia, as of 5 February 2023 at 18:00 WIB (UTC+7), 204,266,655 people had received the first dose of the vaccine and 175,131,893 people had been fully vaccinated; 69,597,474 of them had been inoculated with the booster or the third dose, while 1,585,164 had received the fourth dose. Jakarta has the highest percentage of population fully vaccinated with 103.46%, followed by Bali and Special Region of Yogyakarta with 85.45% and 83.02% respectively.\nDespite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with relatively higher number of vaccination rate and how they changed over time.\n\n\n\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.\nf## The Task\nThe specific tasks of this take-home exercise are as follows:\n\n\n\nCompute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,\nPrepare the monthly vaccination rate maps by using appropriate tmap functions,\nDescribe the spatial patterns revealed by the choropleth maps (not more than 200 words).\n\n\n\n\nWith reference to the vaccination rate maps prepared in ESDA:\n\nCompute local Gi* values of the monthly vaccination rate,\nDisplay the Gi* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value < 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 250 words).\n\n\n\n\nWith reference to the local Gi* values of the vaccination rate maps prepared in the previous section:\n\nPerform Mann-Kendall Test by using the spatio-temporal local Gi* values,\nSelect three sub-districts and describe the temporal trends revealed (not more than 250 words), and\nPrepared a EHSA map of the Gi* values of vaccination rate. The maps should only display the significant (i.e. p-value < 0.05).\nWith reference to the EHSA map prepared, describe the spatial patterns revealed. (not more than 250 words).\n\n\n\n\n\n\n\nFor the purpose of this assignment, data from Riwayat File Vaksinasi DKI Jakarta will be used. Daily vaccination data are provides. You are only required to download either the first day of the month or last day of the month of the study period.\n\n\n\nFor the purpose of this study, DKI Jakarta administration boundary 2019 will be used. The data set can be downloaded at Indonesia Geospatial portal, specifically at this page.\nNote\n\nThe national Projected Coordinates Systems of Indonesia is DGN95 / Indonesia TM-3 zone 54.1.\nExclude all the outer islands from the DKI Jakarta sf data frame, and\nRetain the first nine fields in the DKI Jakarta sf data frame. The ninth field JUMLAH_PEN = Total Population.\nReference was taken from the senior sample submissions for the code for this section, with credit to Megan - https://is415-msty.netlify.app/posts/2021-09-10-take-home-exercise-1/"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#importing-the-geospatial-data",
    "href": "Take-home_Ex2/Take_home_exe2.html#importing-the-geospatial-data",
    "title": "Take Home Exercise 2",
    "section": "1.1 Importing the Geospatial Data",
    "text": "1.1 Importing the Geospatial Data\nThe code chunk below uses st_read() of sf package to import BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA shapefile into R. The imported shapefile will be simple features Object of sf. As we can see, the assigned coordinates system is WGS 84, the ‘World Geodetic System 1984’. In the context of this dataset, this isn’t appropriate: as this is an Indonesian-specific geospatial dataset, we should be using the national CRS of Indonesia, DGN95, the ‘Datum Geodesi Nasional 1995’, ESPG code 23845. st_transform will be used to rectify the coordinate system\n\nbd_jakarta <- st_read(dsn = \"data/Geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\")\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\Harith-oh\\IS415-Harith\\Take-home_Ex2\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nFrom the output message we can see that there are 269 features and 161 fields. The assigned CRS is WGS 84, the ‘World Geodetic System 1984’. This is not right, and will be rectify that later."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#data-pre-processing",
    "href": "Take-home_Ex2/Take_home_exe2.html#data-pre-processing",
    "title": "Take Home Exercise 2",
    "section": "1.2 Data Pre-Processing",
    "text": "1.2 Data Pre-Processing\n\n1.2.1 Check for Missing Values\nNow lets check if there are any missing values\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 2 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.8412 ymin: -6.154036 xmax: 106.8612 ymax: -6.144973\nGeodetic CRS:  WGS 84\n    OBJECT_ID KODE_DESA             DESA   KODE    PROVINSI KAB_KOTA KECAMATAN\n243     25645  31888888     DANAU SUNTER 318888 DKI JAKARTA     <NA>      <NA>\n244     25646  31888888 DANAU SUNTER DLL 318888 DKI JAKARTA     <NA>      <NA>\n    DESA_KELUR JUMLAH_PEN JUMLAH_KK LUAS_WILAY KEPADATAN PERPINDAHA JUMLAH_MEN\n243       <NA>          0         0          0         0          0          0\n244       <NA>          0         0          0         0          0          0\n    PERUBAHAN WAJIB_KTP SILAM KRISTEN KHATOLIK HINDU BUDHA KONGHUCU KEPERCAYAA\n243         0         0     0       0        0     0     0        0          0\n244         0         0     0       0        0     0     0        0          0\n    PRIA WANITA BELUM_KAWI KAWIN CERAI_HIDU CERAI_MATI U0 U5 U10 U15 U20 U25\n243    0      0          0     0          0          0  0  0   0   0   0   0\n244    0      0          0     0          0          0  0  0   0   0   0   0\n    U30 U35 U40 U45 U50 U55 U60 U65 U70 U75 TIDAK_BELU BELUM_TAMA TAMAT_SD SLTP\n243   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n244   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n    SLTA DIPLOMA_I DIPLOMA_II DIPLOMA_IV STRATA_II STRATA_III BELUM_TIDA\n243    0         0          0          0         0          0          0\n244    0         0          0          0         0          0          0\n    APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN NELAYAN AGAMA_DAN PELAJAR_MA\n243          0          0          0         0       0         0          0\n244          0          0          0         0       0         0          0\n    TENAGA_KES PENSIUNAN LAINNYA GENERATED KODE_DES_1 BELUM_ MENGUR_ PELAJAR_\n243          0         0       0      <NA>       <NA>      0       0        0\n244          0         0       0      <NA>       <NA>      0       0        0\n    PENSIUNA_1 PEGAWAI_ TENTARA KEPOLISIAN PERDAG_ PETANI PETERN_ NELAYAN_1\n243          0        0       0          0       0      0       0         0\n244          0        0       0          0       0      0       0         0\n    INDUSTR_ KONSTR_ TRANSP_ KARYAW_ KARYAW1 KARYAW1_1 KARYAW1_12 BURUH BURUH_\n243        0       0       0       0       0         0          0     0      0\n244        0       0       0       0       0         0          0     0      0\n    BURUH1 BURUH1_1 PEMBANT_ TUKANG TUKANG_1 TUKANG_12 TUKANG__13 TUKANG__14\n243      0        0        0      0        0         0          0          0\n244      0        0        0      0        0         0          0          0\n    TUKANG__15 TUKANG__16 TUKANG__17 PENATA PENATA_ PENATA1_1 MEKANIK SENIMAN_\n243          0          0          0      0       0         0       0        0\n244          0          0          0      0       0         0       0        0\n    TABIB PARAJI_ PERANCA_ PENTER_ IMAM_M PENDETA PASTOR WARTAWAN USTADZ JURU_M\n243     0       0        0       0      0       0      0        0      0      0\n244     0       0        0       0      0       0      0        0      0      0\n    PROMOT ANGGOTA_ ANGGOTA1 ANGGOTA1_1 PRESIDEN WAKIL_PRES ANGGOTA1_2\n243      0        0        0          0        0          0          0\n244      0        0        0          0        0          0          0\n    ANGGOTA1_3 DUTA_B GUBERNUR WAKIL_GUBE BUPATI WAKIL_BUPA WALIKOTA WAKIL_WALI\n243          0      0        0          0      0          0        0          0\n244          0      0        0          0      0          0        0          0\n    ANGGOTA1_4 ANGGOTA1_5 DOSEN GURU PILOT PENGACARA_ NOTARIS ARSITEK AKUNTA_\n243          0          0     0    0     0          0       0       0       0\n244          0          0     0    0     0          0       0       0       0\n    KONSUL_ DOKTER BIDAN PERAWAT APOTEK_ PSIKIATER PENYIA_ PENYIA1 PELAUT\n243       0      0     0       0       0         0       0       0      0\n244       0      0     0       0       0         0       0       0      0\n    PENELITI SOPIR PIALAN PARANORMAL PEDAGA_ PERANG_ KEPALA_ BIARAW_ WIRASWAST_\n243        0     0      0          0       0       0       0       0          0\n244        0     0      0          0       0       0       0       0          0\n    LAINNYA_12 LUAS_DESA KODE_DES_3 DESA_KEL_1 KODE_12\n243          0         0       <NA>       <NA>       0\n244          0         0       <NA>       <NA>       0\n                          geometry\n243 MULTIPOLYGON (((106.8612 -6...\n244 MULTIPOLYGON (((106.8504 -6...\n\n\nThere are 2 rows containing ‘NA’ values. However, the data is big, we need to find columns with missing NA values to remove it.\n\nnames(which(colSums(is.na(bd_jakarta))>0))\n\n[1] \"KAB_KOTA\"   \"KECAMATAN\"  \"DESA_KELUR\" \"GENERATED\"  \"KODE_DES_1\"\n[6] \"KODE_DES_3\" \"DESA_KEL_1\"\n\n\nWe can see that there are two particular rows with missing values for KAB_KOTA (City), KECAMATAN (District) and DESA_KELUR (Village).\nHence, we remove rows with NA value in DESA_KELUR. There are other columns with NA present as well, however, since we are only looking at the sub-district level, it is most appropriate to remove DESA_KELUR.\n\nbd_jakarta <- na.omit(bd_jakarta,c(\"DESA_KELUR\"))\n\nTo double check if the rows with missing values are removed\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 0 features and 161 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n  [1] OBJECT_ID  KODE_DESA  DESA       KODE       PROVINSI   KAB_KOTA  \n  [7] KECAMATAN  DESA_KELUR JUMLAH_PEN JUMLAH_KK  LUAS_WILAY KEPADATAN \n [13] PERPINDAHA JUMLAH_MEN PERUBAHAN  WAJIB_KTP  SILAM      KRISTEN   \n [19] KHATOLIK   HINDU      BUDHA      KONGHUCU   KEPERCAYAA PRIA      \n [25] WANITA     BELUM_KAWI KAWIN      CERAI_HIDU CERAI_MATI U0        \n [31] U5         U10        U15        U20        U25        U30       \n [37] U35        U40        U45        U50        U55        U60       \n [43] U65        U70        U75        TIDAK_BELU BELUM_TAMA TAMAT_SD  \n [49] SLTP       SLTA       DIPLOMA_I  DIPLOMA_II DIPLOMA_IV STRATA_II \n [55] STRATA_III BELUM_TIDA APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN \n [61] NELAYAN    AGAMA_DAN  PELAJAR_MA TENAGA_KES PENSIUNAN  LAINNYA   \n [67] GENERATED  KODE_DES_1 BELUM_     MENGUR_    PELAJAR_   PENSIUNA_1\n [73] PEGAWAI_   TENTARA    KEPOLISIAN PERDAG_    PETANI     PETERN_   \n [79] NELAYAN_1  INDUSTR_   KONSTR_    TRANSP_    KARYAW_    KARYAW1   \n [85] KARYAW1_1  KARYAW1_12 BURUH      BURUH_     BURUH1     BURUH1_1  \n [91] PEMBANT_   TUKANG     TUKANG_1   TUKANG_12  TUKANG__13 TUKANG__14\n [97] TUKANG__15 TUKANG__16 TUKANG__17 PENATA     PENATA_    PENATA1_1 \n[103] MEKANIK    SENIMAN_   TABIB      PARAJI_    PERANCA_   PENTER_   \n[109] IMAM_M     PENDETA    PASTOR     WARTAWAN   USTADZ     JURU_M    \n[115] PROMOT     ANGGOTA_   ANGGOTA1   ANGGOTA1_1 PRESIDEN   WAKIL_PRES\n[121] ANGGOTA1_2 ANGGOTA1_3 DUTA_B     GUBERNUR   WAKIL_GUBE BUPATI    \n[127] WAKIL_BUPA WALIKOTA   WAKIL_WALI ANGGOTA1_4 ANGGOTA1_5 DOSEN     \n[133] GURU       PILOT      PENGACARA_ NOTARIS    ARSITEK    AKUNTA_   \n[139] KONSUL_    DOKTER     BIDAN      PERAWAT    APOTEK_    PSIKIATER \n[145] PENYIA_    PENYIA1    PELAUT     PENELITI   SOPIR      PIALAN    \n[151] PARANORMAL PEDAGA_    PERANG_    KEPALA_    BIARAW_    WIRASWAST_\n[157] LAINNYA_12 LUAS_DESA  KODE_DES_3 DESA_KEL_1 KODE_12    geometry  \n<0 rows> (or 0-length row.names)\n\n\n\n\n1.2.2 Transforming Coordinates\nPreviously as mentioned it uses the WGS 84 coordinate system. The data is using a Geographic projected system, however, this is system is not appropriate since we need to use distance and area measures.\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nTherefore, we use st_transform() and not st_set_crs() as st_set_crs() assigns the EPSG code to the data frame. And we need to transform the data frame from geographic to projected coordinate system. We will be using crs=23845 (found from the EPSG for Indonesia).\n\nbd_jakarta <- st_transform(bd_jakarta, 23845)\n\nCheck if CRS has been assigned\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: EPSG:23845 \n  wkt:\nPROJCRS[\"DGN95 / Indonesia TM-3 zone 54.1\",\n    BASEGEOGCRS[\"DGN95\",\n        DATUM[\"Datum Geodesi Nasional 1995\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4755]],\n    CONVERSION[\"Indonesia TM-3 zone 54.1\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",139.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9999,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre.\"],\n        AREA[\"Indonesia - onshore east of 138°E.\"],\n        BBOX[-9.19,138,-1.49,141.01]],\n    ID[\"EPSG\",23845]]\n\n\n\n\n1.2.3 Removal of the Outer Island\nWe have done our basic pre-processing, lets quickly visualize the data\n\nplot(st_geometry(bd_jakarta))\n\n\n\n\nAs we can see from the diagram, bd_jakarta includes both mainland and outer islands. And since we don’t require the outer islands (as per the requirements), we can remove them.\nWe know that the date is grouped by KAB_KOTA (City), KECAMATAN (Sub-District) and DESA_KELUR (Village). Now, lets plot the map and see how we can use KAB_KOTA to remove the outer islands.\n\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\nFrom the map, we can see that all the cities in Jakarta start with ‘Jakarta’ as their prefix and hence, ‘Kepulauan Seribu’ are the other outer islands. When translated in English, the name means ‘Thousand Islands’. Now we know what to remove, and we shall proceed with that.\n\nbd_jakarta <- filter(bd_jakarta, KAB_KOTA != \"KEPULAUAN SERIBU\")\n\nNow, lets double check if the outer islands have been removed.\n\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\n\n\n1.2.4 To retain the first 9 columns as requested\n\nbd_jakarta <- bd_jakarta[, 0:9]\n\n\n\n1.2.5 Renaming columns to English\n\nbd_jakarta <- bd_jakarta %>% \n  dplyr::rename(\n    Object_ID=OBJECT_ID,\n    Village_Code=KODE_DESA, \n    Village=DESA,\n    Code=KODE,\n    Province=PROVINSI, \n    City=KAB_KOTA, \n    District=KECAMATAN, \n    Sub_District=DESA_KELUR,\n    Total_Population=JUMLAH_PEN\n    )"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#importing-eda",
    "href": "Take-home_Ex2/Take_home_exe2.html#importing-eda",
    "title": "Take Home Exercise 2",
    "section": "2.1 Importing EDA",
    "text": "2.1 Importing EDA\nFor this take home exercise 2, we will be working on data from July 2021 to June 2022, as a result we will be having several excel files.\n\njul2021 <- read_xlsx(\"data/Aspatial/Data Vaksinasi Berbasis Kelurahan (31 Juli 2021).xlsx\")\n\nglimpse(jul2021)\n\nRows: 268\nColumns: 27\n$ `KODE KELURAHAN`                             <chr> NA, \"3172051003\", \"317304…\n$ `WILAYAH KOTA`                               <chr> NA, \"JAKARTA UTARA\", \"JAK…\n$ KECAMATAN                                    <chr> NA, \"PADEMANGAN\", \"TAMBOR…\n$ KELURAHAN                                    <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\"…\n$ SASARAN                                      <dbl> 8941211, 23947, 29381, 29…\n$ `BELUM VAKSIN`                               <dbl> 4441501, 12333, 13875, 18…\n$ `JUMLAH\\r\\nDOSIS 1`                          <dbl> 4499710, 11614, 15506, 10…\n$ `JUMLAH\\r\\nDOSIS 2`                          <dbl> 1663218, 4181, 4798, 3658…\n$ `TOTAL VAKSIN\\r\\nDIBERIKAN`                  <dbl> 6162928, 15795, 20304, 14…\n$ `LANSIA\\r\\nDOSIS 1`                          <dbl> 502579, 1230, 2012, 865, …\n$ `LANSIA\\r\\nDOSIS 2`                          <dbl> 440910, 1069, 1729, 701, …\n$ `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN`          <dbl> 943489, 2299, 3741, 1566,…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 1`                  <dbl> 1052883, 3333, 2586, 2837…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 2`                  <dbl> 666009, 2158, 1374, 1761,…\n$ `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN`   <dbl> 1718892, 5491, 3960, 4598…\n$ `GOTONG ROYONG\\r\\nDOSIS 1`                   <dbl> 56660, 78, 122, 174, 71, …\n$ `GOTONG ROYONG\\r\\nDOSIS 2`                   <dbl> 38496, 51, 84, 106, 57, 7…\n$ `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN`    <dbl> 95156, 129, 206, 280, 128…\n$ `TENAGA KESEHATAN\\r\\nDOSIS 1`                <dbl> 76397, 101, 90, 215, 73, …\n$ `TENAGA KESEHATAN\\r\\nDOSIS 2`                <dbl> 67484, 91, 82, 192, 67, 3…\n$ `TENAGA KESEHATAN TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl> 143881, 192, 172, 407, 14…\n$ `TAHAPAN 3\\r\\nDOSIS 1`                       <dbl> 2279398, 5506, 9012, 5408…\n$ `TAHAPAN 3\\r\\nDOSIS 2`                       <dbl> 446028, 789, 1519, 897, 4…\n$ `TAHAPAN 3 TOTAL\\r\\nVAKSIN DIBERIKAN`        <dbl> 2725426, 6295, 10531, 630…\n$ `REMAJA\\r\\nDOSIS 1`                          <dbl> 531793, 1366, 1684, 1261,…\n$ `REMAJA\\r\\nDOSIS 2`                          <dbl> 4291, 23, 10, 1, 1, 8, 6,…\n$ `REMAJA TOTAL\\r\\nVAKSIN DIBERIKAN`           <dbl> 536084, 1389, 1694, 1262,…\n\n\nFrom opening up the excel file till February 2022, the number of columns is 27. However, from March 2022 the number of columns is 34. Upon identifying the difference between the number of columns, the data files from March 2022 has a separate column for 3rd dosage, where has all the data files before that don’t have 3rd dosage column."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#creating-aspatial-data-pre-processing-function",
    "href": "Take-home_Ex2/Take_home_exe2.html#creating-aspatial-data-pre-processing-function",
    "title": "Take Home Exercise 2",
    "section": "2.2 Creating Aspatial Data Pre-Processing Function",
    "text": "2.2 Creating Aspatial Data Pre-Processing Function\nFor take home exercise 2, we don’t require all the columns. Only the following columns are required -\nKODE KELURAHAN (Sub-District Code)\nKELURAHAN (Sub-District)\nSASARAN (Target)\nBELUM VASKIN (Yet to be vaccinated / Not yet vaccinated)\nThis solves the issue of some months having extra columns. However, we need to create an ‘Date’ column that shows the month and year of the observation, which is originally the file name. Each file has the naming convention ’Data Vaksinasi Berbasis Keluarahan (DD Month YYYY).\nWe will be combining the mentioned steps into a function\n\n# takes in an aspatial data filepath and returns a processed output\naspatial_preprocess <- function(filepath){\n  # We have to remove the first row of the file (subheader row) and hence, we use [-1,] to remove it.\n  result_file <- read_xlsx(filepath)[-1,]\n  \n  # We then create the Date Column, the format of our files is: Data Vaksinasi Berbasis Kelurahan (DD Month YYYY)\n  # While the start is technically \"(\", \"(\" is part of a regular expression and leads to a warning message, so we'll use \"Kelurahan\" instead. The [[1]] refers to the first element in the list.\n  # We're loading it as DD-Month-YYYY format\n  # We use the length of the filepath '6' to get the end index (which has our Date)\n  # as such, the most relevant functions are substr (returns a substring) and either str_locate (returns location of substring as an integer matrix) or gregexpr (returns a list of locations of substring)\n  # reference https://stackoverflow.com/questions/14249562/find-the-location-of-a-character-in-string\n  startpoint <- gregexpr(pattern=\"Kelurahan\", filepath)[[1]] + 11\n  \n  result_file$Date <- substr(filepath, startpoint, nchar(filepath)-6)\n  \n  # Retain the Relevant Columns\n  result_file <- result_file %>% \n    select(\"Date\", \n           \"KODE KELURAHAN\", \n           \"KELURAHAN\", \n           \"SASARAN\", \n           \"BELUM VAKSIN\")\n  return(result_file)\n}"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#feed-files-into-aspatial-function",
    "href": "Take-home_Ex2/Take_home_exe2.html#feed-files-into-aspatial-function",
    "title": "Take Home Exercise 2",
    "section": "2.3 Feed files into Aspatial function",
    "text": "2.3 Feed files into Aspatial function\nInstead of manually feeding the files, line by line, we will be using the function list.files() and lapply() to get our process done quicker.\n\n# in the folder 'data/aspatial', find files with the extension '.xlsx' and add it to our fileslist \n# the full.names=TRUE prepends the directory path to the file names, giving a relative file path - otherwise, only the file names (not the paths) would be returned \n# reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/list.files.html\nfileslist <-list.files(path = \"data/Aspatial\", pattern = \"*.xlsx\", full.names=TRUE)\n\n# afterwards, for every element in fileslist, apply aspatial_process function\ndflist <- lapply(seq_along(fileslist), function(x) aspatial_preprocess(fileslist[x]))\n\nWe will then convert the dflist into an actual dataframe with ldply() using the below code\n\nvaccination_jakarta <- ldply(dflist, data.frame)\n\nNow, lets take a look into our data\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date           <chr> \"27 Februari 2022\", \"27 Februari 2022\", \"27 Februari 20…\n$ KODE.KELURAHAN <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"3175031003\",…\n$ KELURAHAN      <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"BAMBU…\n$ SASARAN        <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 47898, …\n$ BELUM.VAKSIN   <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 3772, 7…"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#formatting-date-column",
    "href": "Take-home_Ex2/Take_home_exe2.html#formatting-date-column",
    "title": "Take Home Exercise 2",
    "section": "2.4 Formatting Date Column",
    "text": "2.4 Formatting Date Column\nThe Dates are in Bahasa Indonesia, and hence, we need to translate them to English for ease of use. However, since the values in Date column were derived from sub-strings, they are in a string format and thus, first need to be converted to datetime.\n\n# parses the 'Date' column into Month(Full Name)-YYYY datetime objects\n# reference: https://stackoverflow.com/questions/53380650/b-y-date-conversion-gives-na\n\n# locale=\"ind\" means that the locale has been set as Indonesia\nSys.setlocale(locale=\"ind\")\n\n[1] \"LC_COLLATE=Indonesian_Indonesia.1252;LC_CTYPE=Indonesian_Indonesia.1252;LC_MONETARY=Indonesian_Indonesia.1252;LC_NUMERIC=C;LC_TIME=Indonesian_Indonesia.1252\"\n\n\n\nvaccination_jakarta$Date <- c(vaccination_jakarta$Date) %>% \n  as.Date(vaccination_jakarta$Date, format =\"%d %B %Y\")\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date           <date> 2022-02-27, 2022-02-27, 2022-02-27, 2022-02-27, 2022-0~\n$ KODE.KELURAHAN <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"3175031003\",~\n$ KELURAHAN      <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"BAMBU~\n$ SASARAN        <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 47898, ~\n$ BELUM.VAKSIN   <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 3772, 7~"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#rename-columns-into-english",
    "href": "Take-home_Ex2/Take_home_exe2.html#rename-columns-into-english",
    "title": "Take Home Exercise 2",
    "section": "2.5 Rename columns into English",
    "text": "2.5 Rename columns into English\n\n# renames the columns in the style New_Name = OLD_NAME\nvaccination_jakarta <- vaccination_jakarta %>% \n  dplyr::rename(\n    Date=Date,\n    Sub_District_Code=KODE.KELURAHAN,\n    Sub_District=KELURAHAN, \n    Target=SASARAN, \n    Not_Yet_Vaccinated=BELUM.VAKSIN\n    )\n\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date               <date> 2022-02-27, 2022-02-27, 2022-02-27, 2022-02-27, 20~\n$ Sub_District_Code  <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"31750310~\n$ Sub_District       <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"B~\n$ Target             <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 478~\n$ Not_Yet_Vaccinated <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 377~"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#further-data-processing",
    "href": "Take-home_Ex2/Take_home_exe2.html#further-data-processing",
    "title": "Take Home Exercise 2",
    "section": "2.6 Further data processing",
    "text": "2.6 Further data processing\nFurther perform any pre-processing to check out for anything we might have missed.\n\nvaccination_jakarta[rowSums(is.na(vaccination_jakarta))!=0,]\n\n[1] Date               Sub_District_Code  Sub_District       Target            \n[5] Not_Yet_Vaccinated\n<0 rows> (or 0-length row.names)\n\n\nFrom the output, we can see there are no missing values."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#initial-exploratory-data-analysis",
    "href": "Take-home_Ex2/Take_home_exe2.html#initial-exploratory-data-analysis",
    "title": "Take Home Exercise 2",
    "section": "3.1 Initial Exploratory Data Analysis",
    "text": "3.1 Initial Exploratory Data Analysis\nWe have both our Geospatial and Aspatial data, we need to join them. However, we need to first find a common header to join them.\n\ncolnames(bd_jakarta)\n\n [1] \"Object_ID\"        \"Village_Code\"     \"Village\"          \"Code\"            \n [5] \"Province\"         \"City\"             \"District\"         \"Sub_District\"    \n [9] \"Total_Population\" \"geometry\"        \n\n\n\ncolnames(vaccination_jakarta)\n\n[1] \"Date\"               \"Sub_District_Code\"  \"Sub_District\"      \n[4] \"Target\"             \"Not_Yet_Vaccinated\"\n\n\nWe can see that both have Sub_District and hence we can join them by the Sub_District and Sub_District_Code.\n\n# joins vaccination_jakarta to jakarta based on Sub_District and  Sub_District_Code\ncombined_jakarta <- left_join(bd_jakarta, vaccination_jakarta,\n                              by=c(\n                                \"Village_Code\"=\"Sub_District_Code\", \n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nSubcategorize the data into ‘Target population to be Vaccinated’ , ‘Not Yet Vaccinated Population’ and ‘Total Population’\n\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target Count\")\n\nnot_yet_vaccinated = tm_shape(combined_jakarta)+\n  tm_fill(\"Not_Yet_Vaccinated\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Not Yet Vaccinated Count\")\n\ntotal_population = tm_shape(combined_jakarta)+\n  tm_fill(\"Total_Population\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Total Population\")\n\ntmap_arrange(target, not_yet_vaccinated, total_population)\n\n\n\n\nThere seems to be still be a ‘Missing’ value in the Target and Not_Yet_Vaccinated maps. Even though, when we had previously checked for missing values, it didn’t show any missing values. However, we shall double check again.\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 0 features and 9 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n [1] Object_ID        Village_Code     Village          Code            \n [5] Province         City             District         Sub_District    \n [9] Total_Population geometry        \n<0 rows> (or 0-length row.names)\n\n\n\nvaccination_jakarta[rowSums(is.na(vaccination_jakarta))!=0,]\n\n[1] Date               Sub_District_Code  Sub_District       Target            \n[5] Not_Yet_Vaccinated\n<0 rows> (or 0-length row.names)\n\n\nThere are no missing values in our dataframes. Therefore, the most likely reasons for the missing values must be due to mismatched values when we perform the left-join of the Geospatial and Aspatial data."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#finding-mismatched-sub-district-records",
    "href": "Take-home_Ex2/Take_home_exe2.html#finding-mismatched-sub-district-records",
    "title": "Take Home Exercise 2",
    "section": "3.2 Finding mismatched sub-district records",
    "text": "3.2 Finding mismatched sub-district records\nSince, we had conducted left-join using the Sub-District, there must be a mismatch in the naming of the subdistricts. Lets check it by looking at the unique subdistrict names in both bd_jakarta and vaccination_jakarta\n\njakarta_subdistrict <- c(bd_jakarta$Sub_District)\nvaccination_subdistrict <- c(vaccination_jakarta$Sub_District)\n\nunique(jakarta_subdistrict[!(jakarta_subdistrict %in% vaccination_subdistrict)])\n\n[1] \"KRENDANG\"             \"RAWAJATI\"             \"TENGAH\"              \n[4] \"BALEKAMBANG\"          \"PINANGRANTI\"          \"JATIPULO\"            \n[7] \"PALMERIAM\"            \"KRAMATJATI\"           \"HALIM PERDANA KUSUMA\"\n\n\n\nunique(vaccination_subdistrict[!(vaccination_subdistrict %in% jakarta_subdistrict)])\n\n [1] \"BALE KAMBANG\"          \"HALIM PERDANA KUSUMAH\" \"JATI PULO\"            \n [4] \"KAMPUNG TENGAH\"        \"KERENDANG\"             \"KRAMAT JATI\"          \n [7] \"PAL MERIAM\"            \"PINANG RANTI\"          \"PULAU HARAPAN\"        \n[10] \"PULAU KELAPA\"          \"PULAU PANGGANG\"        \"PULAU PARI\"           \n[13] \"PULAU TIDUNG\"          \"PULAU UNTUNG JAWA\"     \"RAWA JATI\"            \n\n\nFrom above there are same names in both but are just written in different ways. However, there are 6 words in the vaccination_subdistrict which are not in the jakarta_subdistrict. We need to take a look into that after we first correct the mismatched values.\n\n# initialise a dataframe of our cases vs bd subdistrict spelling\nspelling <- data.frame(\n  Aspatial_Cases=c(\"BALE KAMBANG\", \"HALIM PERDANA KUSUMAH\", \"JATI PULO\", \"KAMPUNG TENGAH\", \"KERENDANG\", \"KRAMAT JATI\", \"PAL MERIAM\", \"PINANG RANTI\", \"RAWA JATI\"),\n  Geospatial_BD=c(\"BALEKAMBAG\", \"HALIM PERDANA KUSUMA\", \"JATIPULO\", \"TENGAH\", \"KRENDANG\", \"KRAMATJATI\", \"PALMERIAM\", \"PINANGRANTI\", \"RAWAJATI\")\n  )\n\n# with dataframe a input, outputs a kable\nlibrary(knitr)\nlibrary(kableExtra)\nkable(spelling, caption=\"Mismatched Records\") %>%\n  kable_material(\"hover\", latex_options=\"scale_down\")\n\n\n\nMismatched Records\n \n  \n    Aspatial_Cases \n    Geospatial_BD \n  \n \n\n  \n    BALE KAMBANG \n    BALEKAMBAG \n  \n  \n    HALIM PERDANA KUSUMAH \n    HALIM PERDANA KUSUMA \n  \n  \n    JATI PULO \n    JATIPULO \n  \n  \n    KAMPUNG TENGAH \n    TENGAH \n  \n  \n    KERENDANG \n    KRENDANG \n  \n  \n    KRAMAT JATI \n    KRAMATJATI \n  \n  \n    PAL MERIAM \n    PALMERIAM \n  \n  \n    PINANG RANTI \n    PINANGRANTI \n  \n  \n    RAWA JATI \n    RAWAJATI \n  \n\n\n\n\n\nAs we can see these records have the same name, except that there is no standardization. Therefore, there is a mismatch between them. Let’s correct this mismatch\n\n# We are replacing the mistmatched values in jakarta with the correct value\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'BALEKAMBANG'] <- 'BALE KAMBANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'HALIM PERDANA KUSUMA'] <- 'HALIM PERDANA KUSUMAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'JATIPULO'] <- 'JATI PULO'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KALI BARU'] <- 'KALIBARU'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'TENGAH'] <- 'KAMPUNG TENGAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRAMATJATI'] <- 'KRAMAT JATI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRENDANG'] <- 'KERENDANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PALMERIAM'] <- 'PAL MERIAM'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PINANGRANTI'] <- 'PINANG RANTI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'RAWAJATI'] <- 'RAWA JATI'\n\nThere are 6 subdistrict names that we say in vaccination_jakarta which were not present in jakarta. This ideally suggests that these districts are not a part of Jakarta, Therefore we need to remove them.\n\nvaccination_jakarta <- vaccination_jakarta[!(vaccination_jakarta$Sub_District==\"PULAU HARAPAN\" | vaccination_jakarta$Sub_District==\"PULAU KELAPA\" | vaccination_jakarta$Sub_District==\"PULAU PANGGANG\" | vaccination_jakarta$Sub_District==\"PULAU PARI\" | vaccination_jakarta$Sub_District==\"PULAU TIDUNG\" | vaccination_jakarta$Sub_District==\"PULAU UNTUNG JAWA\"), ]"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#rejoin-exploratory-data-analysis",
    "href": "Take-home_Ex2/Take_home_exe2.html#rejoin-exploratory-data-analysis",
    "title": "Take Home Exercise 2",
    "section": "3.3 Rejoin Exploratory Data Analysis",
    "text": "3.3 Rejoin Exploratory Data Analysis\n\n# joins vaccination_jakarta to bd_jakarta based on Sub_District and  Sub_District_Code\ncombined_jakarta <- left_join(bd_jakarta, vaccination_jakarta,\n                              by=c(\n                                \"Village_Code\"=\"Sub_District_Code\", \n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nCheck if there are any further NA values\n\ncombined_jakarta[rowSums(is.na(combined_jakarta))!=0,]\n\nSimple feature collection with 0 features and 12 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n [1] Object_ID          Village_Code       Village            Code              \n [5] Province           City               District           Sub_District      \n [9] Total_Population   Date               Target             Not_Yet_Vaccinated\n[13] geometry          \n<0 rows> (or 0-length row.names)\n\n\nRelook the data into ‘Target population to be Vaccinated’ , ‘Not Yet Vaccinated Population’ and ‘Total Population’\n\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target Count\")\n\nnot_yet_vaccinated = tm_shape(combined_jakarta)+\n  tm_fill(\"Not_Yet_Vaccinated\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Not Yet Vaccinated Count\")\n\ntotal_population = tm_shape(combined_jakarta)+\n  tm_fill(\"Total_Population\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Total Population\")\n\ntmap_arrange(target, not_yet_vaccinated, total_population)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#convert-dataframe-to-sf",
    "href": "Take-home_Ex2/Take_home_exe2.html#convert-dataframe-to-sf",
    "title": "Take Home Exercise 2",
    "section": "4.1 Convert dataframe to SF",
    "text": "4.1 Convert dataframe to SF\n\ncombined_jakarta <- st_as_sf(combined_jakarta)\n\n# need to join our previous dataframes with the geospatial data to ensure that geometry column is present\nvaccination_rate <- vaccination_rate%>% left_join(bd_jakarta, by=c(\"Sub_District\"=\"Sub_District\"))\nvaccination_rate <- st_as_sf(vaccination_rate)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#jenks-choropleth-mapping",
    "href": "Take-home_Ex2/Take_home_exe2.html#jenks-choropleth-mapping",
    "title": "Take Home Exercise 2",
    "section": "5.1 Jenks Choropleth Mapping",
    "text": "5.1 Jenks Choropleth Mapping\n\n# using the jenks method, with 6 classes for human eye\ntmap_mode(\"plot\")\ntm_shape(vaccination_rate)+\n  tm_fill(\"2021-07-31\", \n          n= 6,\n          style = \"jenks\", \n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Vaccination Rate in July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlot for all 12 months. Adopt a helper function to help us do it.\n\n# input: the dataframe and the variable name - in this case, the month \n# with style=\"jenks\" for the jenks classification method\njenks_plot <- function(df, varname) {\n  tm_shape(vaccination_rate) +\n    tm_polygons() +\n  tm_shape(df) +\n    tm_fill(varname, \n          n= 6,\n          style = \"jenks\", \n          title = \"Vaccination Rate\") +\n    tm_layout(main.title = varname,\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          legend.height = 0.45, \n          legend.width = 0.35,\n          frame = TRUE) +\n    tm_borders(alpha = 0.5)\n}\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2021-07-31\"),\n             jenks_plot(vaccination_rate, \"2021-08-31\"),\n             jenks_plot(vaccination_rate, \"2021-09-30\"),\n             jenks_plot(vaccination_rate, \"2021-10-31\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2021-11-30\"),\n             jenks_plot(vaccination_rate, \"2021-12-31\"),\n             jenks_plot(vaccination_rate, \"2022-01-31\"),\n             jenks_plot(vaccination_rate, \"2022-02-27\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2022-03-31\"),\n             jenks_plot(vaccination_rate, \"2022-04-30\"),\n             jenks_plot(vaccination_rate, \"2022-05-31\"),\n             jenks_plot(vaccination_rate, \"2022-06-30\"))\n\n\n\n\nObservations from the plotted map\nEach plotted map has been arranged monthly and has it own relative vaccination rate. As observed, the colours turns darker over time. As the ranges and gradually grow larger over time, more people are getting vaccinated.\nBy looking at the early stages between July 2021 and October 2021, there is a darkly-coloured cluster around the north of Jakarta which include KAMAL MUARA and HALIM PERDANA KUSUMAH sub-district with the highest vaccination rate.\nAs for other sub districts between (November 2021 ~ February 2022, other sub districts have darken in colour and the HALIM PERDANA KUSUMAH still remains the sub-district with the highest vaccination rate.\nIn the later stages of vaccination from March 2022, more sub-districts have lower vaccination rate (lighter colour) especially for the most of the sub-districts in the North and West. However, HALIM PERDANA KUSUMAH still remains the sub-district with the highest vaccination rate.\nChecking for sub-districts with highest vaccination rate according to month\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-07-31`)]\n\n[1] \"KAMAL MUARA\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-08-31`)]\n\n[1] \"KAMAL MUARA\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-09-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-10-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-11-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-12-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-01-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-02-27`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-03-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-04-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-05-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-06-31`)]\n\ncharacter(0)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#calculation-of-local-gi-of-monthly-vaccination-rate",
    "href": "Take-home_Ex2/Take_home_exe2.html#calculation-of-local-gi-of-monthly-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "6.1 Calculation of Local GI* of monthly vaccination rate",
    "text": "6.1 Calculation of Local GI* of monthly vaccination rate\n\n# Make new vaccination attribute table with Date, Sub_District, Target, Not_Yet_Vaccinated\nvaccination_table <- combined_jakarta %>% select(10, 8, 11, 12) %>% st_drop_geometry()\n\n# Adding a new field for Vaccination_Rate\nvaccination_table$Vaccination_Rate <- ((vaccination_table$Target - vaccination_table$Not_Yet_Vaccinated) / vaccination_table$Target) *100\n\n# Vaccination attribute table with just Date, Sub_District, Vaccination_Rate\nvaccination_table <- tibble(vaccination_table %>% select(1,2,5))"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#create-time-series-cube",
    "href": "Take-home_Ex2/Take_home_exe2.html#create-time-series-cube",
    "title": "Take Home Exercise 2",
    "section": "6.2 Create Time Series Cube",
    "text": "6.2 Create Time Series Cube\n\nvaccination_rate_st <- spacetime(vaccination_table, bd_jakarta,\n                          .loc_col = \"Sub_District\",\n                          .time_col = \"Date\")\n\nVerify if vaccination_rate_st is indeed a space-time cube by using the is_spacetime_cube() of sfdep package.\n\nis_spacetime_cube(vaccination_rate_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#deriving-spatial-weights",
    "href": "Take-home_Ex2/Take_home_exe2.html#deriving-spatial-weights",
    "title": "Take Home Exercise 2",
    "section": "6.3 Deriving Spatial Weights",
    "text": "6.3 Deriving Spatial Weights\nCalculation of local Gi* weights will be done. However, before that we need derive the spatial weights. The below code chunk is used to identify neighbors and derive an inverse distance weights.\n\nvaccination_rate_nb <- vaccination_rate_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale=1,\n                                  alpha=1),\n         .before=1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\nNote that\n\nactivate() is used to activate the geometry context\nmutate() is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\nThe dataset provided has neighbours and weights for each time slicing\n\nhead(vaccination_rate_nb)\n\n# A tibble: 6 x 5\n  Date       Sub_District  Vaccination_Rate nb        wt       \n  <date>     <chr>                    <dbl> <list>    <list>   \n1 2021-07-31 KEAGUNGAN                 53.3 <int [6]> <dbl [6]>\n2 2021-07-31 GLODOK                    61.6 <int [7]> <dbl [7]>\n3 2021-07-31 HARAPAN MULIA             49.7 <int [6]> <dbl [6]>\n4 2021-07-31 CEMPAKA BARU              46.7 <int [7]> <dbl [7]>\n5 2021-07-31 PASAR BARU                59.3 <int [9]> <dbl [9]>\n6 2021-07-31 KARANG ANYAR              52.2 <int [7]> <dbl [7]>\n\n\nset.seed() will be use before performing simulation to ensure that the computation is reproducible. When a random number generator is used, the results can be different each time the code is run, which makes it difficult to reproduce results. By setting the seed to a specific value (e.g., set.seed(1234)), the same random numbers will be generated each time the code is run, making the results reproducible and consistent.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#calculation-of-gi-value",
    "href": "Take-home_Ex2/Take_home_exe2.html#calculation-of-gi-value",
    "title": "Take Home Exercise 2",
    "section": "6.4 Calculation of GI* value",
    "text": "6.4 Calculation of GI* value\nThe calculation of the Gi* value for each sub-district where we group by date\n\ngi_values <- vaccination_rate_nb |>\n  group_by(Date) |>\n  mutate(gi_values = local_gstar_perm(\n    Vaccination_Rate, nb, wt, nsim=99)) |>\n      tidyr::unnest(gi_values)\n\ngi_values\n\n# A tibble: 3,132 x 13\n# Groups:   Date [12]\n   Date       Sub_Di~1 Vacci~2 nb    wt    gi_star    e_gi  var_gi p_value p_sim\n   <date>     <chr>      <dbl> <lis> <lis>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 2021-07-31 KEAGUNG~    53.3 <int> <dbl>   2.44  0.00383 2.13e-8 1.46e-2  0.02\n 2 2021-07-31 GLODOK      61.6 <int> <dbl>   3.85  0.00384 1.56e-8 1.18e-4  0.02\n 3 2021-07-31 HARAPAN~    49.7 <int> <dbl>   0.309 0.00382 2.20e-8 7.57e-1  0.84\n 4 2021-07-31 CEMPAKA~    46.7 <int> <dbl>  -1.05  0.00383 1.53e-8 2.96e-1  0.34\n 5 2021-07-31 PASAR B~    59.3 <int> <dbl>   2.71  0.00383 1.38e-8 6.82e-3  0.02\n 6 2021-07-31 KARANG ~    52.2 <int> <dbl>   1.67  0.00382 2.17e-8 9.49e-2  0.1 \n 7 2021-07-31 MANGGA ~    51.6 <int> <dbl>   1.35  0.00384 1.80e-8 1.77e-1  0.22\n 8 2021-07-31 PETOJO ~    47.2 <int> <dbl>  -0.179 0.00382 1.92e-8 8.58e-1  0.96\n 9 2021-07-31 SENEN       54.4 <int> <dbl>   1.51  0.00382 1.20e-8 1.32e-1  0.1 \n10 2021-07-31 BUNGUR      52.8 <int> <dbl>   0.797 0.00385 1.54e-8 4.25e-1  0.48\n# ... with 3,122 more rows, 3 more variables: p_folded_sim <dbl>,\n#   skewness <dbl>, kurtosis <dbl>, and abbreviated variable names\n#   1: Sub_District, 2: Vaccination_Rate"
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#visualise-the-monthly-values-of-gi",
    "href": "Take-home_Ex2/Take_home_exe2.html#visualise-the-monthly-values-of-gi",
    "title": "Take Home Exercise 2",
    "section": "6.5 Visualise the monthly values of GI*",
    "text": "6.5 Visualise the monthly values of GI*\nTo be able to visualise the Gi* values of the monthly vaccination rate, we need to join it with combined_jakarta, to be able to plot the Gi* values on the map. As the gi_values do not have any coordinates\n\njakarta_gi_values <- combined_jakarta %>%\n  left_join(gi_values)\n\njakarta_gi_values\n\nSimple feature collection with 3132 features and 23 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3644275 ymin: 663887.8 xmax: -3606237 ymax: 701380.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\nFirst 10 features:\n   Object_ID Village_Code   Village   Code    Province          City   District\n1      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n2      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n3      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n4      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n5      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n6      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n7      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n8      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n9      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n10     25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n   Sub_District Total_Population       Date Target Not_Yet_Vaccinated\n1     KEAGUNGAN            21609 2022-02-27  17387               2755\n2     KEAGUNGAN            21609 2022-04-30  17387               2593\n3     KEAGUNGAN            21609 2022-06-30  17387               2553\n4     KEAGUNGAN            21609 2021-11-30  17387               3099\n5     KEAGUNGAN            21609 2021-09-30  17387               4203\n6     KEAGUNGAN            21609 2021-08-31  17387               6054\n7     KEAGUNGAN            21609 2021-12-31  17387               2924\n8     KEAGUNGAN            21609 2022-01-31  17387               2783\n9     KEAGUNGAN            21609 2021-07-31  17387               8126\n10    KEAGUNGAN            21609 2022-03-31  17387               2675\n   Vaccination_Rate                      nb\n1          84.15483 1, 2, 39, 152, 158, 166\n2          85.08656 1, 2, 39, 152, 158, 166\n3          85.31662 1, 2, 39, 152, 158, 166\n4          82.17634 1, 2, 39, 152, 158, 166\n5          75.82677 1, 2, 39, 152, 158, 166\n6          65.18088 1, 2, 39, 152, 158, 166\n7          83.18284 1, 2, 39, 152, 158, 166\n8          83.99379 1, 2, 39, 152, 158, 166\n9          53.26393 1, 2, 39, 152, 158, 166\n10         84.61494 1, 2, 39, 152, 158, 166\n                                                                             wt\n1  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n2  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n3  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n4  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n5  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n6  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n7  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n8  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n9  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n10 0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n    gi_star        e_gi       var_gi     p_value p_sim p_folded_sim    skewness\n1  2.478859 0.003831172 8.419778e-10 0.013180341  0.02         0.01 -0.03194341\n2  2.825027 0.003827108 7.494269e-10 0.004727670  0.04         0.02 -0.06356613\n3  2.856078 0.003828377 6.980492e-10 0.004289104  0.02         0.01  0.09797571\n4  2.400280 0.003831389 1.665572e-09 0.016382514  0.02         0.01 -0.75383323\n5  2.340497 0.003839767 1.969413e-09 0.019258078  0.02         0.01 -0.45706127\n6  2.697283 0.003843766 4.229906e-09 0.006990777  0.02         0.01  0.19649652\n7  2.737341 0.003831004 9.440138e-10 0.006193804  0.02         0.01 -0.13372915\n8  2.601179 0.003833862 7.779184e-10 0.009290410  0.02         0.01 -0.51408533\n9  2.442508 0.003830045 2.133669e-08 0.014585591  0.02         0.01 -0.40723702\n10 2.631489 0.003829259 7.973859e-10 0.008501151  0.04         0.02  0.23980167\n      kurtosis                       geometry\n1  -0.81316154 MULTIPOLYGON (((-3626874 69...\n2   0.55021722 MULTIPOLYGON (((-3626874 69...\n3  -0.50749002 MULTIPOLYGON (((-3626874 69...\n4   0.54499553 MULTIPOLYGON (((-3626874 69...\n5   0.28836755 MULTIPOLYGON (((-3626874 69...\n6   0.03187466 MULTIPOLYGON (((-3626874 69...\n7  -0.02562365 MULTIPOLYGON (((-3626874 69...\n8   0.07882560 MULTIPOLYGON (((-3626874 69...\n9   0.28865884 MULTIPOLYGON (((-3626874 69...\n10 -0.20275849 MULTIPOLYGON (((-3626874 69...\n\n\nWe will proceed with visualizing the first month (July 2021). We will be plotting both the Gi* value and the p-value of Gi* for the Vaccination Rates.\nAs per take home exercise 2 requirement, we will only be plotting the significant p-value < 0.05\n\ngi_value_plot <- function(date, title) {\n  gi_star_map = tm_shape(filter(jakarta_gi_values, Date == date)) +\n    tm_fill(\"gi_star\") +\n    tm_borders(alpha=0.5) +\n    tm_view(set.zoom.limits = c(6,8)) +\n    tm_layout(main.title = paste(\"Gi* values for vaccination rates in\", title), main.title.size=0.8)\n\n  p_value_map = tm_shape(filter(jakarta_gi_values, Date == date)) +\n    tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) +\n    tm_borders(alpha=0.5) + \n    tm_layout(main.title = paste(\"p-values of Gi* for vaccination rates in\", title), main.title.size=0.8)\n\n  tmap_arrange(gi_star_map, p_value_map)\n}\n\nPlotting Gi* for all 12 months\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-07-31\", \"July 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-08-31\", \"August 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-09-30\", \"September 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-10-31\", \"October 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-11-30\", \"November 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-12-31\", \"December 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-01-31\", \"January 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-02-27\", \"February 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-03-31\", \"March 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-04-30\", \"April 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-05-31\", \"May 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-06-30\", \"June 2022\")\n\n\n\n\nStatistical conclusion\nThe p-value represents the probability of observing a clustering. A significant p-value < 0.05 suggests that an observed pattern is unlikely to have occurred by chance and may indicate the presence of a spatial process. When Gi* value > 0, it indicates sub-districts with a higher vaccination rate than average. We can view the number of sub districts with p-value < 0.05 with the code below\n\nno_of_subdistricts_freq = filter(jakarta_gi_values, p_sim < 0.05)\nas.data.frame(table(no_of_subdistricts_freq$Sub_District))\n\n                      Var1 Freq\n1                    ANCOL    1\n2             BALE KAMBANG   11\n3              BALI MESTER    4\n4                     BARU    9\n5               BATU AMPAR   12\n6          BENDUNGAN HILIR   10\n7              BIDARA CINA    9\n8      CEMPAKA PUTIH BARAT    1\n9                  CIBUBUR   10\n10                CIGANJUR   10\n11               CIJANTUNG   10\n12                  CIKINI    1\n13                  CIKOKO    8\n14          CILANDAK BARAT    3\n15          CILANDAK TIMUR    7\n16               CILILITAN    4\n17               CILINCING   10\n18                CIPAYUNG    6\n19                 CIPEDAK   10\n20  CIPINANG BESAR SELATAN    3\n21    CIPINANG BESAR UTARA    8\n22       CIPINANG CEMPEDAK   12\n23          CIPINANG MUARA    2\n24                 CIPULIR    2\n25                 CIRACAS    1\n26             DUREN SAWIT    1\n27            DURI SELATAN    1\n28                   GALUR    2\n29                  GAMBIR    5\n30                  GEDONG    3\n31                  GELORA    7\n32                  GLODOK   12\n33              GONDANGDIA    7\n34          GROGOL SELATAN    1\n35            GROGOL UTARA    1\n36   GUNUNG SAHARI SELATAN    1\n37     GUNUNG SAHARI UTARA    1\n38   HALIM PERDANA KUSUMAH   11\n39               JAGAKARSA    9\n40              JATINEGARA    1\n41         JATINEGARA KAUM    1\n42           JEMBATAN LIMA    1\n43              JOHAR BARU    1\n44                KALIBARU   10\n45                KALISARI    9\n46                   KAMAL    1\n47             KAMAL MUARA    1\n48            KAMPUNG BALI   12\n49          KAMPUNG MELAYU    3\n50            KAMPUNG RAWA    3\n51          KAMPUNG TENGAH   12\n52             KAPUK MUARA    2\n53           KARET TENGSIN    4\n54              KAYU MANIS    1\n55               KEAGUNGAN   12\n56               KEBAGUSAN    5\n57            KEBON KACANG   12\n58            KEBON MELATI   12\n59             KEBON SIRIH    7\n60              KELAPA DUA    3\n61        KELAPA DUA WETAN   11\n62     KELAPA GADING BARAT    2\n63     KELAPA GADING TIMUR    7\n64                 KLENDER    2\n65                    KOJA    2\n66                  KRAMAT    2\n67             KRAMAT JATI    3\n68                   LAGOA    9\n69           LENTENG AGUNG    9\n70            LUBANG BUAYA    5\n71             MALAKA JAYA    4\n72             MALAKA SARI    1\n73            MANGGA BESAR   12\n74       MANGGARAI SELATAN    8\n75                  MAPHAR    3\n76                 MARUNDA    1\n77                 MELAWAI    2\n78                 MENTENG    7\n79                  MUNJUL    9\n80              PAL MERIAM    1\n81                PAPANGGO    1\n82              PASAR BARU    2\n83           PASAR MANGGIS    2\n84          PEGANGSAAN DUA    3\n85           PEJATEN TIMUR    3\n86                 PEKAYON    8\n87            PENGGILINGAN    1\n88             PENJARINGAN    2\n89              PETAMBURAN   12\n90               PETOGOGAN    2\n91          PETOJO SELATAN    7\n92            PETOJO UTARA    2\n93        PETUKANGAN UTARA    1\n94            PINANG RANTI    1\n95               PINANGSIA    3\n96           PISANGAN BARU    2\n97          PISANGAN TIMUR    1\n98            PONDOK BAMBU    4\n99             PONDOK KOPI    2\n100            PONDOK LABU    9\n101            PULO GADUNG    2\n102                RAGUNAN    7\n103               RAMBUTAN    1\n104     RAWA BADAK SELATAN    1\n105       RAWA BADAK UTARA    4\n106             RAWA BARAT    1\n107             RAWA BUNGA    9\n108             ROA MALAKA    1\n109                 SELONG    2\n110           SEMPER BARAT    5\n111           SEMPER TIMUR    4\n112                SENAYAN    2\n113                  SENEN    2\n114              SRENGSENG    1\n115        SRENGSENG SAWAH    9\n116       SUKABUMI SELATAN    4\n117         SUKABUMI UTARA    3\n118           SUNTER AGUNG    1\n119            SUNTER JAYA    4\n120           TANAH SEREAL    5\n121           TANAH TINGGI    2\n122                 TANGKI   10\n123          TANJUNG BARAT    6\n124            TEBET BARAT   10\n125            TEBET TIMUR   10\n126                 TOMANG    1\n127             TUGU UTARA    5\n128                ULUJAMI    1\n\n\nFrom the table above, there are 128 sub-districts who have a significant vaccination rate p-value < 0.05 at least once during the period of 12 months. Those sub-districts that have double digits frequency have a significant p value throughout the entire 12 months."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#mann-kendall-test",
    "href": "Take-home_Ex2/Take_home_exe2.html#mann-kendall-test",
    "title": "Take Home Exercise 2",
    "section": "7.1 Mann-Kendall Test",
    "text": "7.1 Mann-Kendall Test\n\n7.1.1 KEAGUNGAN\n\nKeagungan <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"KEAGUNGAN\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = Keagungan, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nKeagungan %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau     sl     S     D  varS\n  <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 0.455 0.0467    30  66.0  213.\n\n\nThe p-value is 0.046 which is < 0.05 hence p-value is significant. Therefore, this is a fluctuating upward significant trend.\n\n\n7.1.2 HARAPAN MULIA\n\nHARAPAN_MULIA <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"HARAPAN MULIA\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = HARAPAN_MULIA, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nHARAPAN_MULIA %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n     tau    sl     S     D  varS\n   <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.0909 0.732     6  66.0  213.\n\n\nThe p-value is 0.73 which is > 0.05 hence p-value is not significant. Therefore, this is an downward insignificant trend.\n\n\n7.1.3 ULUJAMI\n\nulujami <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"ULUJAMI\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = ulujami, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nulujami %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau      sl     S     D  varS\n  <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 0.667 0.00319    44  66.0  213.\n\n\nThe p-value is 0.003 which is < 0.05 hence p-value is significant. Therefore, this is an upward but significant trend."
  },
  {
    "objectID": "Take-home_Ex2/Take_home_exe2.html#ehsa-map-of-the-gi-value",
    "href": "Take-home_Ex2/Take_home_exe2.html#ehsa-map-of-the-gi-value",
    "title": "Take Home Exercise 2",
    "section": "7.2 EHSA map of the Gi* value",
    "text": "7.2 EHSA map of the Gi* value\nFor us to find the significant hot and cold spots, there is a need to conduct the Mann Kendall test on all the subdistricts out there. Therefore, the group_by() function will be used for all subdistricts.\n\nehsa <- gi_values %>%\n  group_by(Sub_District) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\nShow significant top 10 emerging hot/cold spots area\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nemerging\n\n# A tibble: 10 x 6\n   Sub_District             tau        sl     S     D  varS\n   <chr>                  <dbl>     <dbl> <dbl> <dbl> <dbl>\n 1 PETOJO UTARA          -0.970 0.0000156   -64  66.0  213.\n 2 KAYU MANIS             0.970 0.0000156    64  66.0  213.\n 3 JATINEGARA KAUM        0.939 0.0000287    62  66.0  213.\n 4 PISANGAN BARU          0.939 0.0000287    62  66.0  213.\n 5 PASAR BARU            -0.939 0.0000288   -62  66.0  213.\n 6 DUKUH                  0.909 0.0000521    60  66.0  213.\n 7 KAMAL MUARA           -0.909 0.0000521   -60  66.0  213.\n 8 KAPUK MUARA           -0.909 0.0000521   -60  66.0  213.\n 9 KEBON KELAPA          -0.909 0.0000521   -60  66.0  213.\n10 GUNUNG SAHARI SELATAN -0.879 0.0000928   -58  66.0  213.\n\n\nemerging_hotspot_analysis() of sfdep package will be used to perform EHSA analysis. It takes a spacetime object x (i.e. vaccination_rate_st), and the quoted name of the variable of interest (i.e. Vaccinaton Rate) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default.\n\nehsa <- emerging_hotspot_analysis(\n  x = vaccination_rate_st,\n  .var = \"Vaccination_Rate\",\n  k = 1,\n  nsim = 99\n)\n\nVisualisation of distribution\n\nggplot(data = ehsa,\n       aes(x=classification, fill=classification)) + \n  geom_bar()\n\n\n\n\nThe barchart above shows that sporadic hot spots class has the highest numbers.\nLeft join of combine jakarta and ehsa together\n\njakarta_ehsa <- bd_jakarta %>%\n  left_join(ehsa, by = c(\"Sub_District\" = \"location\"))\n\nVisualisation of classification using tmap\n\n# We use the filter to filter out values with p-value < 0.05\njakarta_ehsa_sig <- jakarta_ehsa  %>%\n  filter(p_value < 0.05)\ntmap_mode(\"plot\")\ntm_shape(jakarta_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(jakarta_ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nObservation:\nOscilating coldspot is spread out evenly in Jakarta which can be found to be more around the border and in the central of Jakarta. At the same time, the oscillating hotspot is lesser than spordiac coldspot. Similar to oscilating coldspot, there is also a large number of spordiac hotspot spread out evenly around Jakarta. From the map, the patterns are not obvious and the sub districts shaded in grey are of p value > 0.05 which means that the sub districts are insignificant.\nEnd of take home exercise 2"
  },
  {
    "objectID": "Take-home_Ex3/Take_home_Ex3.html",
    "href": "Take-home_Ex3/Take_home_Ex3.html",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.\nConventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, Geographical Weighted Models were introduced for calibrating predictive model for housing resale prices.\n\n\n\nIn this take-home exercise, you are tasked to predict HDB resale prices at the sub-market level (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. You are also required to compare the performance of the conventional OLS method versus the geographical weighted methods.\n\n\n\nFor the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat and transaction period should be from 1st January 2021 to 31st December 2022. The test data should be January and February 2023 resale prices."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_Ex08.html",
    "title": "In-class Exercise 8: Building hedonic pricing model with gwr",
    "section": "",
    "text": "pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, readr)\n\n13.5 Geospatial Data Wrangling 13.5.1 Importing geospatial data The geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/Geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Harith-oh\\IS415-Harith\\In-class_Ex\\data\\Geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nReading layer MP14_SUBZONE_WEB_PL' from data sourceD:gdsa’ using driver `ESRI Shapefile’ Simple feature collection with 323 features and 15 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33 Projected CRS: SVY21 The report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n13.5.2 Updating CRS information The code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCoordinate Reference System: User input: EPSG:3414 wkt: PROJCRS[“SVY21 / Singapore TM”, BASEGEOGCRS[“SVY21”, DATUM[“SVY21”, ELLIPSOID[“WGS 84”,6378137,298.257223563, LENGTHUNIT[“metre”,1]]], PRIMEM[“Greenwich”,0, ANGLEUNIT[“degree”,0.0174532925199433]], ID[“EPSG”,4757]], CONVERSION[“Singapore Transverse Mercator”, METHOD[“Transverse Mercator”, ID[“EPSG”,9807]], PARAMETER[“Latitude of natural origin”,1.36666666666667, ANGLEUNIT[“degree”,0.0174532925199433], ID[“EPSG”,8801]], PARAMETER[“Longitude of natural origin”,103.833333333333, ANGLEUNIT[“degree”,0.0174532925199433], ID[“EPSG”,8802]], PARAMETER[“Scale factor at natural origin”,1, SCALEUNIT[“unity”,1], ID[“EPSG”,8805]], PARAMETER[“False easting”,28001.642, LENGTHUNIT[“metre”,1], ID[“EPSG”,8806]], PARAMETER[“False northing”,38744.572, LENGTHUNIT[“metre”,1], ID[“EPSG”,8807]]], CS[Cartesian,2], AXIS[“northing (N)”,north, ORDER[1], LENGTHUNIT[“metre”,1]], AXIS[“easting (E)”,east, ORDER[2], LENGTHUNIT[“metre”,1]], USAGE[ SCOPE[“Cadastre, engineering survey, topographic mapping.”], AREA[“Singapore - onshore and offshore.”], BBOX[1.13,103.59,1.47,104.07]], ID[“EPSG”,3414]] Notice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n xmin      ymin      xmax      ymax \n2667.538 15748.721 56396.440 50256.334\n13.6 Aspatial Data Wrangling 13.6.1 Importing the aspatial data The condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\ncondo_resale = read_csv(\"data/Aspatial/Condo_resale_2015.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198 Next, summary() of base R is used to display the summary statistics of cond_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n13.6.2 Converting aspatial data frame into a sf object Currently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLI…¹ AREA_…²   AGE PROX_…³ PROX_…⁴ PROX_…⁵ PROX_…⁶ PROX_…⁷ PROX_…⁸\n     <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   118635 3000000     309    30    7.94   0.166   2.52     6.62   1.77   0.0584\n2   288420 3880000     290    32    6.61   0.280   1.93     7.51   0.545  0.616 \n3   267833 3325000     248    33    6.90   0.429   0.502    6.46   0.378  0.141 \n4   258380 4250000     127     7    4.04   0.395   1.99     4.91   1.68   0.382 \n5   467169 1400000     145    28   11.8    0.119   1.12     6.41   0.565  0.461 \n6   466472 1320000     139    22   10.3    0.125   0.789    5.09   0.781  0.0994\n# … with 12 more variables: PROX_MRT <dbl>, PROX_PARK <dbl>,\n#   PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>, and abbreviated variable names\n#   ¹​SELLING_PRICE, ²​AREA_SQM, ³​PROX_CBD, ⁴​PROX_CHILDCARE, ⁵​PROX_ELDERLYCARE,\n#   ⁶​PROX_URA_GROWTH_AREA, ⁷​PROX_HAWKER_MARKET, ⁸​PROX_KINDERGARTEN\n\n\nNotice that the output is in point feature data frame.\n\ncorrplot::corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n13.7 Exploratory Data Analysis (EDA) In the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n13.7.1 EDA using statistical graphics We can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n13.7.2 Multiple Histogram Plots distribution of variables In this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html",
    "href": "In-class_Ex/In-class_Ex09.html",
    "title": "In-class Exercise 9",
    "section": "",
    "text": "pacman::p_load(sf, spdep, GWmodel, SpatialML, tidyverse, tmap, ggpubr, olsrr, devtools,\n              tidymodels, rsample)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#preparing-data",
    "href": "In-class_Ex/In-class_Ex09.html#preparing-data",
    "title": "In-class Exercise 9",
    "section": "Preparing Data",
    "text": "Preparing Data\n\nReading data file to rds\n\nmdata <- read_rds(\"data/Aspatial/mdata.rds\")\n\n\n\nData Sampling\n\nset.seed(1234)\nresale_split <- initial_split(mdata,prop = 6.5/10,)\ntrain_data <- training(resale_split)\ntest_data <- testing(resale_split)\n\n\nprice_mlr <- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n                data=train_data)\n\n\nsummary(price_mlr)\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              107601.073  10601.261  10.150  < 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  < 2e-16 ***\nstorey_order              14299.298    339.115  42.167  < 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  < 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  < 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  < 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  < 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  < 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  < 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  < 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: < 2.2e-16\n\n\n\nwrite_rds(price_mlr, \"data/rds/price_mlr.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#preparing-coordinate-data",
    "href": "In-class_Ex/In-class_Ex09.html#preparing-coordinate-data",
    "title": "In-class Exercise 9",
    "section": "Preparing coordinate data",
    "text": "Preparing coordinate data\n\nExtracting coordinates data\nThe code chunks below extract the x,y coordinates of the full, training and test data sets.\n\ncoords <- st_coordinates(mdata)\ncoords_train <- st_coordinates(train_data)\ncoords_test <- st_coordinates(test_data)\n\n\ncoords_train <- write_rds(coords_train, \"data/rds/coords_train.rds\")\n\ncoords_test <- write_rds(coords_test,\"data/rds/coords_test.rds\")\n\n\n\nDropping geometry field\nFirst, we will drop geometry column of the sf data frame by using st_drop_geometry() of sf package\n\ntrain_data <- train_data %>%\n  st_drop_geometry()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#calibrating-random-forest",
    "href": "In-class_Ex/In-class_Ex09.html#calibrating-random-forest",
    "title": "In-class Exercise 9",
    "section": "Calibrating Random Forest",
    "text": "Calibrating Random Forest\npreforming random forest calibration by using [ranger]\n\nset.seed(1234)\nrf <- ranger(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n                data=train_data)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09.html#calibrating-geographically-weighted-random-forest-model",
    "href": "In-class_Ex/In-class_Ex09.html#calibrating-geographically-weighted-random-forest-model",
    "title": "In-class Exercise 9",
    "section": "Calibrating Geographically Weighted Random Forest Model",
    "text": "Calibrating Geographically Weighted Random Forest Model\nIn this section, you will learn how to calibrate a predict model by\nThe code chunk beow calibrate a geographic random forest model by using ‘grf()’ f SpatialML package.\n\nset.seed(1234)\n\ngwRF_adaptive <- grf(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,\n                dframe=train_data,\n                bw=55,\n                kernel=\"adaptive\",\n                coords=coords_train)\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       700081018 \nR squared (OOB):                  0.9515468 \n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.376510e+12             1.413229e+13             2.991844e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.312697e+13             7.017513e+12             5.506719e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.446857e+12             4.825986e+12             4.173165e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.879598e+12             1.028775e+12             1.701318e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.564038e+12             7.214027e+12 \n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-236112.0  -13033.7     444.4     593.8   14831.5  358041.7 \n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-79279.83  -3510.70     54.56     50.98   3909.85  83074.08 \n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 401562922035 18210850992 41426270899\nstorey_order             302736445 243728744368 16368419468 23620589843\nremaining_lease_mths     696564138 546463600727 34119912443 70328183398\nPROX_CBD                  55173040 382484896335 12154563393 29293290548\nPROX_ELDERLYCARE          45182031 344081962746 10597657883 24546405941\nPROX_HAWKER               43516026 342597797419 10551807020 23408387903\nPROX_MRT                  54234551 299075025906  9873129985 21055852211\nPROX_PARK                 49919822 322633843469  9353956995 19517077658\nPROX_MALL                 43296133 433263607933 11247374493 27537334970\nPROX_SUPERMARKET          52665827 417310417234 10802122271 26572460731\nWITHIN_350M_KINDERGARTEN         0 186468064682  2848177740 12928886968\nWITHIN_350M_CHILDCARE            0 255236737234  5526292324 18109971102\nWITHIN_350M_BUS                  0 193828795378  4747552546 11886064288\nWITHIN_1KM_PRISCH                0 178360608427  1778262602  7163381668"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10.html",
    "href": "In-class_Ex/In-class_Ex10.html",
    "title": "In-class Exercise 10",
    "section": "",
    "text": "Getting started loading of R packages\n2.1 Importing Geospatial Data\n2.2 Update CRS information\n2.3 Cleaning and updating attributes fields of geospatial data\nThe code chunk below uses read_cvs() of readr package to import OD_Matrix.csv into RStudio. The imported object is a tibble data.frame called ODMatrix."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10.html#computing-distance-matrix-optional",
    "href": "In-class_Ex/In-class_Ex10.html#computing-distance-matrix-optional",
    "title": "In-class Exercise 10",
    "section": "Computing Distance Matrix (optional)",
    "text": "Computing Distance Matrix (optional)\n\neldercare_coord <- st_coordinates(eldercare)\nhexagon_coord <- st_coordinates(hexagons)\n\n\n#EucMatrix <- SpatialAcc:distance(hexagon_coord,\n                           #      eldercare_coord,\n                           #      type = \"euclidean\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10.html#computing-hansens-accessbility",
    "href": "In-class_Ex/In-class_Ex10.html#computing-hansens-accessbility",
    "title": "In-class Exercise 10",
    "section": "4.1 Computing Hansen’s accessbility",
    "text": "4.1 Computing Hansen’s accessbility\nNow, we ready to compute Hansen’s accessibility by using ac() of SpatialAcc package. Before getting started, you are encourage to read the arguments of the function at least once in order to ensure that the required inputs are available.\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\nacc_Hansen <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\ncolnames(acc_Hansen) <- \"accHansen\"\n\nNext, we will convert the data table into tibble format by using the code chunk below.\n\nacc_Hansen <- tbl_df(acc_Hansen)\n\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature data frame. The output is called hexagon_Hansen.\n\nhexagon_Hansen <- bind_cols(hexagons, acc_Hansen)\n\n\nacc_Hansen <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 0.5, \n                            family = \"Hansen\"))\n\ncolnames(acc_Hansen) <- \"accHansen\"\nacc_Hansen <- tbl_df(acc_Hansen)\nhexagon_Hansen <- bind_cols(hexagons, acc_Hansen)\n\n#5 Visualising Hansen’s accessibility ##5.1 Extracting map extend Firstly, we will extract the extend of hexagons simple feature data frame by by using st_bbox() of sf package.\n\nmapex <- st_bbox(hexagons)\n\nThe code chunk below uses a collection of mapping fucntions of tmap package to create a high cartographic quality accessibility to eldercare centre in Singapore.\n\ntmap_mode(\"plot\")\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)"
  }
]